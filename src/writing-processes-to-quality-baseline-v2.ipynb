{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":59291,"databundleVersionId":6678907,"sourceType":"competition"},{"sourceId":6940442,"sourceType":"datasetVersion","datasetId":3985804},{"sourceId":7004245,"sourceType":"datasetVersion","datasetId":4026671},{"sourceId":7004282,"sourceType":"datasetVersion","datasetId":4026595},{"sourceId":150936354,"sourceType":"kernelVersion"}],"dockerImageVersionId":30559,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Linking Writing Processes to Writing Quality: Baseline - Electric Boogaloo\n\nThis notebook is my second iteration of a public baseline for the Writing Processes to Quality competition.\n\n### **Please upvote this notebook before forking it!**\n\n## References\n\nI used these as my references. **Please go upvote these as well!**\n\n- https://www.kaggle.com/code/hengzheng/link-writing-simple-lgbm-baseline (original baseline)\n- https://www.kaggle.com/code/mcpenguin/writing-processes-to-quality-baseline (my v1 baseline)\n- https://www.kaggle.com/code/olyatsimboy/towards-tf-idf-in-logs-features (TF-IDF features)\n- https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs ({SPACE} features)\n- https://www.kaggle.com/code/hiarsl/feature-engineering-sentence-paragraph-features (Essay features)\n- https://www.kaggle.com/code/alexryzhkov/lgbm-and-nn-on-sentences (LightAutoML)\n- https://www.kaggle.com/code/kawaiicoderuwu/essay-contructor (initial essay constructor)\n- https://www.kaggle.com/code/yuriao/fast-essay-constructor (fast essay constructor)\n\n## Changes\n\n### Encapsulation\n\nI have encapsulated much of the code into classes, which makes it easier to isolate logic, which adheres to the Single Responsibility Principle as much as possible. \n\n### Visualizations\n\nI have added graphs to help analyze the performance of our models. Feel free to iterate on these.\n\n### Feature Engineering\n\nWe use the new [fast essay constructor](https://www.kaggle.com/code/yuriao/fast-essay-constructor) by @yuriao to speed up feature engineering. We also add new features:\n- the number of words whose length exceed a given value N;\n- the number of sentences whose length exceed a given value N;\n- more activity/event counts;\n- the inclusion of both the TF-IDF and regular versions of counts for activity and events.\n\nTo save time, we also include the pre-engineered features for the training logs in a separate dataset and provide an option to load these in if desired.\n\n### Modelling\n\nWe use tweaked LGBM parameters and the same setup for LightAutoML as the 0.586 notebook. As the LightAutoML was trained on GPU, we will need to run the notebook using GPU as well.\n\nWe will use an ensemble of 6 models using hill climbing: LightGBM, CatBoost, LightAutoML (DenseLight), Ridge, Lasso and Random Forest.","metadata":{}},{"cell_type":"markdown","source":"# Set Global Configuration Options","metadata":{}},{"cell_type":"code","source":"class CONFIG:\n    '''\n    > General Options\n    '''\n    # global seed\n    seed = 42\n    # the number of samples to use for testing purposes\n    # if None, we use the full dataset\n    samples_testing = None #None\n    # max rows to display for pandas dataframes\n    display_max_rows = 200\n    # name of the response variate we are trying to predict\n    response_variate = 'score'\n    # minimum value for response variate\n    min_possible_response_value = 0.5\n    # maximum value for response variate\n    max_possible_response_value = 6.0\n    \n    '''\n    > Feature Engineering Options\n    '''\n    # whether to use pre feature engineered data or not\n    use_pre_fe_data = True\n    # fe data saved path\n    pre_fe_data_filepath = '/kaggle/input/writing-quality-baseline-v2-train-data/feat_eng_train_feats.csv'\n    \n    '''\n    > Preprocessing Options\n    '''\n    # number of folds to split the data for CV\n    num_folds = 10\n    \n    '''\n    > Modelling + Training Options\n    '''\n    # the names of the models to use\n    # either a list of model names, or 'all', in which case all models are used\n    model_names = 'all'\n    # number of trials to use for early stopping\n    num_trials_early_stopping = 50\n    # model path for lightautoml\n    lightautoml_model_path = '/kaggle/input/writing-quality-baseline-v2-lightautoml/denselight.model'\n    # oof preds path for lightautoml\n    lightautoml_oof_preds_path = '/kaggle/input/writing-quality-baseline-v2-lightautoml/denselight_oof_preds'\n    \n    '''\n    > Post-Modelling Options\n    '''\n    # number of most important features to display\n    # for feature importances plots\n    num_features_to_display = 50","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:43:57.183351Z","iopub.execute_input":"2023-11-19T17:43:57.183689Z","iopub.status.idle":"2023-11-19T17:43:57.196969Z","shell.execute_reply.started":"2023-11-19T17:43:57.183662Z","shell.execute_reply":"2023-11-19T17:43:57.196064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"markdown","source":"## Download LightAutoML v0.3.8 and Pandas v2.0.3","metadata":{}},{"cell_type":"code","source":"!pip install --no-index -Uq --find-links=/kaggle/input/writing-quality-pip-install-with-lightautoml lightautoml==0.3.8\n!pip install --no-index -Uq --find-links=/kaggle/input/writing-quality-pip-install-with-lightautoml pandas==2.0.3","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:43:57.198899Z","iopub.execute_input":"2023-11-19T17:43:57.199427Z","iopub.status.idle":"2023-11-19T17:44:38.523814Z","shell.execute_reply.started":"2023-11-19T17:43:57.1994Z","shell.execute_reply":"2023-11-19T17:44:38.522675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Other Libraries","metadata":{}},{"cell_type":"code","source":"import warnings\n\nimport os\nimport gc\nimport re\nimport random\nfrom collections import Counter, defaultdict\nimport pprint\nimport pickle\nimport time\nimport copy\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.autonotebook import tqdm\n\n# from gensim.models import Word2Vec\nfrom sklearn.preprocessing import LabelEncoder, PowerTransformer, RobustScaler, FunctionTransformer\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD, PCA\nfrom sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cluster import KMeans\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.impute import SimpleImputer\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nimport optuna","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-19T17:44:38.525125Z","iopub.execute_input":"2023-11-19T17:44:38.52544Z","iopub.status.idle":"2023-11-19T17:44:43.632471Z","shell.execute_reply.started":"2023-11-19T17:44:38.525411Z","shell.execute_reply":"2023-11-19T17:44:43.631475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set Some Options","metadata":{}},{"cell_type":"code","source":"tqdm.pandas()\n\nsns.set_style(\"whitegrid\")\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', CONFIG.display_max_rows)\nwarnings.simplefilter('ignore')\n\nrandom.seed(CONFIG.seed)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:44:43.634801Z","iopub.execute_input":"2023-11-19T17:44:43.635095Z","iopub.status.idle":"2023-11-19T17:44:43.640778Z","shell.execute_reply.started":"2023-11-19T17:44:43.63507Z","shell.execute_reply":"2023-11-19T17:44:43.639794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"%%time\nINPUT_DIR = '/kaggle/input/linking-writing-processes-to-writing-quality'\n\ntrain_logs = pd.read_csv(f'{INPUT_DIR}/train_logs.csv')\ntrain_scores = pd.read_csv(f'{INPUT_DIR}/train_scores.csv')\ntest_logs = pd.read_csv(f'{INPUT_DIR}/test_logs.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:44:43.642573Z","iopub.execute_input":"2023-11-19T17:44:43.642869Z","iopub.status.idle":"2023-11-19T17:44:58.014147Z","shell.execute_reply.started":"2023-11-19T17:44:43.642846Z","shell.execute_reply":"2023-11-19T17:44:58.013189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Subsample Data (If Specified)","metadata":{}},{"cell_type":"code","source":"if CONFIG.samples_testing is not None:\n    ids = list(train_logs[\"id\"].unique())\n    sample_ids = random.sample(ids, CONFIG.samples_testing)\n    train_logs = train_logs[train_logs[\"id\"].isin(sample_ids)]","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:44:58.015323Z","iopub.execute_input":"2023-11-19T17:44:58.015607Z","iopub.status.idle":"2023-11-19T17:44:58.023347Z","shell.execute_reply.started":"2023-11-19T17:44:58.015582Z","shell.execute_reply":"2023-11-19T17:44:58.022442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Looking At Data","metadata":{}},{"cell_type":"code","source":"train_logs.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:44:58.024581Z","iopub.execute_input":"2023-11-19T17:44:58.025245Z","iopub.status.idle":"2023-11-19T17:44:58.049386Z","shell.execute_reply.started":"2023-11-19T17:44:58.025211Z","shell.execute_reply":"2023-11-19T17:44:58.048513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_scores.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:44:58.050483Z","iopub.execute_input":"2023-11-19T17:44:58.050767Z","iopub.status.idle":"2023-11-19T17:44:58.059871Z","shell.execute_reply.started":"2023-11-19T17:44:58.050743Z","shell.execute_reply":"2023-11-19T17:44:58.058905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_logs.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:44:58.060977Z","iopub.execute_input":"2023-11-19T17:44:58.061239Z","iopub.status.idle":"2023-11-19T17:44:58.075865Z","shell.execute_reply.started":"2023-11-19T17:44:58.061216Z","shell.execute_reply":"2023-11-19T17:44:58.074991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## Essay Constructor\n\nWe encapsulate the functionality of the essay construction into a class, which allows for easy reusability.\n\nFunction taken from https://www.kaggle.com/code/yuriao/fast-essay-constructor.","metadata":{}},{"cell_type":"code","source":"class EssayConstructor:\n    \n    def processingInputs(self,currTextInput):\n        # Where the essay content will be stored\n        essayText = \"\"\n        # Produces the essay\n        for Input in currTextInput.values:\n            # Input[0] = activity\n            # Input[1] = cursor_position\n            # Input[2] = text_change\n            # Input[3] = id\n            # If activity = Replace\n            if Input[0] == 'Replace':\n                # splits text_change at ' => '\n                replaceTxt = Input[2].split(' => ')\n                # DONT TOUCH\n                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n                continue\n\n            # If activity = Paste    \n            if Input[0] == 'Paste':\n                # DONT TOUCH\n                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n                continue\n\n            # If activity = Remove/Cut\n            if Input[0] == 'Remove/Cut':\n                # DONT TOUCH\n                essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n                continue\n\n            # If activity = Move...\n            if \"M\" in Input[0]:\n                # Gets rid of the \"Move from to\" text\n                croppedTxt = Input[0][10:]              \n                # Splits cropped text by ' To '\n                splitTxt = croppedTxt.split(' To ')              \n                # Splits split text again by ', ' for each item\n                valueArr = [item.split(', ') for item in splitTxt]              \n                # Move from [2, 4] To [5, 7] = (2, 4, 5, 7)\n                moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n                # Skip if someone manages to activiate this by moving to same place\n                if moveData[0] != moveData[2]:\n                    # Check if they move text forward in essay (they are different)\n                    if moveData[0] < moveData[2]:\n                        # DONT TOUCH\n                        essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n                    else:\n                        # DONT TOUCH\n                        essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n                continue                \n                \n            # If activity = input\n            # DONT TOUCH\n            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n        return essayText\n            \n            \n    def getEssays(self,df):\n        # Copy required columns\n        textInputDf = copy.deepcopy(df[['id', 'activity', 'cursor_position', 'text_change']])\n        # Get rid of text inputs that make no change\n        textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']     \n        # construct essay, fast \n        tqdm.pandas()\n        essay=textInputDf.groupby('id')[['activity','cursor_position', 'text_change']].progress_apply(lambda x: self.processingInputs(x))      \n        # to dataframe\n        essayFrame=essay.to_frame().reset_index()\n        essayFrame.columns=['id','essay']\n        # Returns the essay series\n        return essayFrame","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:44:58.080217Z","iopub.execute_input":"2023-11-19T17:44:58.080881Z","iopub.status.idle":"2023-11-19T17:44:58.096966Z","shell.execute_reply.started":"2023-11-19T17:44:58.080844Z","shell.execute_reply":"2023-11-19T17:44:58.096118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessor Class\n\nThe original preprocessor comes from my 0.604 baseline notebook. I have added the new {SPACE} features and essay aggregation features into the preprocessor as well, which may have been separate in the other public notebooks.","metadata":{}},{"cell_type":"code","source":"# nth percentile function for agg\ndef percentile(n):\n    def percentile_(x):\n        return x.quantile(n/100)\n    percentile_.__name__ = 'pct_{:02.0f}'.format(n)\n    return percentile_\n\ndef q1(x):\n    return x.quantile(0.25)\n\ndef q3(x):\n    return x.quantile(0.75)\n\nclass Preprocessor:\n    def __init__(self, seed):\n        self.seed = seed\n        \n        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n        self.text_changes_dict = {\n            'q': 'q', \n            ' ': 'space', \n            'NoChange': 'NoChange', \n            '.': 'full_stop', \n            ',': 'comma', \n            '\\n': 'newline', \n            \"'\": 'single_quote', \n            '\"': 'double_quote', \n            '-': 'dash', \n            '?': 'question_mark', \n            ';': 'semicolon', \n            '=': 'equals', \n            '/': 'slash', \n            '\\\\': 'double_backslash', \n            ':': 'colon'\n        }\n        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n        self.gaps = [1, 2, 3, 5, 10, 20, 50, 70, 100]\n        self.percentiles = [5, 10, 25, 50, 75, 90, 95]\n        self.percentiles_cols = [percentile(n) for n in self.percentiles]\n        self.aggregations = ['mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', pd.DataFrame.kurt, 'sum']\n        self.idf = defaultdict(float)\n        \n        self.essay_constructor = EssayConstructor()\n    \n    def get_essay_aggregations(self, essay_df):\n        cols_to_drop = ['essay']\n        # Total essay length\n        essay_df['essay_len'] = essay_df['essay'].apply(lambda x: len(x))\n        essay_df = essay_df.drop(columns=cols_to_drop)\n        return essay_df\n    \n    def split_essays_into_words(self, essay_df):\n        essay_df['word'] = essay_df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n        essay_df = essay_df.explode('word')\n        # Word length (number of characters in word)\n        essay_df['word_len'] = essay_df['word'].apply(lambda x: len(x))\n        essay_df = essay_df[essay_df['word_len'] != 0]\n        return essay_df\n    \n    def compute_word_aggregations(self, word_df):\n        word_agg_df = word_df[['id','word_len']].groupby(['id']).agg(self.aggregations)\n        word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n        word_agg_df['id'] = word_agg_df.index\n        # New features: computing the # of words whose length exceed word_l\n        for word_l in [5, 6, 7, 8, 9, 10, 11, 12]:\n            word_agg_df[f'word_len_ge_{word_l}_count'] = word_df[word_df['word_len'] >= word_l].groupby(['id']).count().iloc[:, 0]\n            word_agg_df[f'word_len_ge_{word_l}_count'] = word_agg_df[f'word_len_ge_{word_l}_count'].fillna(0)\n        word_agg_df = word_agg_df.reset_index(drop=True)\n        return word_agg_df\n    \n    def split_essays_into_sentences(self, essay_df):\n        essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n        essay_df = essay_df.explode('sent')\n        essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n        # Number of characters in sentences\n        essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n        # Number of words in sentences\n        essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n        essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n        return essay_df\n\n    def compute_sentence_aggregations(self, sent_df):\n        sent_agg_df = sent_df[['id','sent_len','sent_word_count']].groupby(['id']).agg(self.aggregations)\n        sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n        sent_agg_df['id'] = sent_agg_df.index\n        # New features: computing the # of sentences whose (character) length exceed sent_l\n        for sent_l in [50, 60, 75, 100]:\n            sent_agg_df[f'sent_len_ge_{sent_l}_count'] = sent_df[sent_df['sent_len'] >= sent_l].groupby(['id']).count().iloc[:, 0]\n            sent_agg_df[f'sent_len_ge_{sent_l}_count'] = sent_agg_df[f'sent_len_ge_{sent_l}_count'].fillna(0)\n        sent_agg_df = sent_agg_df.reset_index(drop=True)\n        return sent_agg_df\n\n    def split_essays_into_paragraphs(self, essay_df):\n        essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n        essay_df = essay_df.explode('paragraph')\n        # Number of characters in paragraphs\n        essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n        # Number of sentences in paragraphs\n        essay_df['paragraph_sent_count'] = essay_df['paragraph'].apply(lambda x: len(x.split('\\\\.|\\\\?|\\\\!')))\n        # Number of words in paragraphs\n        essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n        essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n        return essay_df\n\n    def compute_paragraph_aggregations(self, paragraph_df):\n        paragraph_agg_df = paragraph_df[['id','paragraph_len', 'paragraph_sent_count', 'paragraph_word_count']].groupby(['id']).agg(self.aggregations)\n        paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n        paragraph_agg_df['id'] = paragraph_agg_df.index\n        paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n        return paragraph_agg_df\n        \n    def activity_counts(self, df):\n        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df['activity'].values):\n            items = list(Counter(li).items())\n            di = dict()\n            for k in self.activities:\n                di[k] = 0\n            # make dictionary entry for \"move from X to Y\"\n            di[\"move_to\"] = 0\n            \n            for item in items:\n                k, v = item[0], item[1]\n                if k in di:\n                    di[k] = v\n                else:\n                    # we can do this because there are no missing values\n                    di[\"move_to\"] += v\n            ret.append(di)\n        \n        ret = pd.DataFrame(ret)\n        # using tfidf\n        ret_tfidf = pd.DataFrame(ret)\n        # returning counts as is\n        ret_normal = pd.DataFrame(ret)\n        \n        tfidf_cols = [f'activity_{act}_tfidf_count' for act in ret.columns]\n        normal_cols = [f'activity_{act}_normal_count' for act in ret.columns]\n        \n        ret_tfidf.columns = tfidf_cols\n        ret_normal.columns = normal_cols\n        \n        '''\n        Credit: https://www.kaggle.com/code/olyatsimboy/towards-tf-idf-in-logs-features\n        '''\n        cnts = ret_tfidf.sum(1)\n\n        for col in tfidf_cols:\n            if col in self.idf.keys():\n                idf = self.idf[col]\n            else:\n                idf = df.shape[0] / (ret_tfidf[col].sum() + 1)\n                idf = np.log(idf)\n                self.idf[col] = idf\n\n            ret_tfidf[col] = 1 + np.log(ret_tfidf[col] / cnts)\n            ret_tfidf[col] *= idf\n        \n        ret_agg = pd.concat([ret_tfidf, ret_normal], axis=1)\n        return ret_agg\n\n    def event_counts(self, df, colname):\n        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df[colname].values):\n            items = list(Counter(li).items())\n            di = dict()\n            for k in self.events:\n                di[k] = 0\n            for item in items:\n                k, v = item[0], item[1]\n                if k in di:\n                    di[k] = v\n            ret.append(di)\n            \n        ret = pd.DataFrame(ret)\n        # using tfidf\n        ret_tfidf = pd.DataFrame(ret)\n        # returning counts as is\n        ret_normal = pd.DataFrame(ret)\n        \n        tfidf_cols = [f'{colname}_{event}_tfidf_count' for event in ret.columns]\n        normal_cols = [f'{colname}_{event}_normal_count' for event in ret.columns]\n        \n        ret_tfidf.columns = tfidf_cols\n        ret_normal.columns = normal_cols\n        \n        '''\n        Credit: https://www.kaggle.com/code/olyatsimboy/towards-tf-idf-in-logs-features\n        '''\n        cnts = ret_tfidf.sum(1)\n\n        for col in tfidf_cols:\n            if col in self.idf.keys():\n                idf = self.idf[col]\n            else:\n                idf = df.shape[0] / (ret_tfidf[col].sum() + 1)\n                idf = np.log(idf)\n                self.idf[col] = idf\n\n            ret_tfidf[col] = 1 + np.log(ret_tfidf[col] / cnts)\n            ret_tfidf[col] *= idf\n        \n        ret_agg = pd.concat([ret_tfidf, ret_normal], axis=1)\n        return ret_agg\n\n    def text_change_counts(self, df):\n        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df['text_change'].values):\n            items = list(Counter(li).items())\n            di = dict()\n            for k in self.text_changes_dict.keys():\n                di[k] = 0\n            for item in items:\n                k, v = item[0], item[1]\n                if k in di:\n                    di[k] = v\n            ret.append(di)\n            \n        ret = pd.DataFrame(ret)\n        # using tfidf\n        ret_tfidf = pd.DataFrame(ret)\n        # returning counts as is\n        ret_normal = pd.DataFrame(ret)\n        \n        tfidf_cols = [f'text_change_{self.text_changes_dict[txt_change]}_tfidf_count' for txt_change in ret.columns]\n        normal_cols = [f'text_change_{self.text_changes_dict[txt_change]}_normal_count' for txt_change in ret.columns]\n        \n        ret_tfidf.columns = tfidf_cols\n        ret_normal.columns = normal_cols\n        \n        '''\n        Credit: https://www.kaggle.com/code/olyatsimboy/towards-tf-idf-in-logs-features\n        '''\n        cnts = ret_tfidf.sum(1)\n\n        for col in tfidf_cols:\n            if col in self.idf.keys():\n                idf = self.idf[col]\n            else:\n                idf = df.shape[0] / (ret_tfidf[col].sum() + 1)\n                idf = np.log(idf)\n                self.idf[col] = idf\n\n            ret_tfidf[col] = 1 + np.log(ret_tfidf[col] / cnts)\n            ret_tfidf[col] *= idf\n        \n        ret_agg = pd.concat([ret_tfidf, ret_normal], axis=1)\n        return ret_agg\n    \n    def match_punctuations(self, df):\n        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df['down_event'].values):\n            cnt = 0\n            items = list(Counter(li).items())\n            for item in items:\n                k, v = item[0], item[1]\n                if k in self.punctuations:\n                    cnt += v\n            ret.append(cnt)\n        ret = pd.DataFrame({'punct_cnt': ret})\n        return ret\n\n    # Credit: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs/notebook\n    def make_space_features(self, df):\n        df['up_time_lagged'] = df.groupby('id')['up_time'].shift(1).fillna(df['down_time'])\n        df['time_diff'] = abs(df['down_time'] - df['up_time_lagged']) / 1000\n\n        group = df.groupby('id')['time_diff']\n        largest_lantency = group.max()\n        smallest_lantency = group.min()\n        median_lantency = group.median()\n        initial_pause = df.groupby('id')['down_time'].first() / 1000\n        pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x < 1)).sum())\n        pauses_1_sec = group.apply(lambda x: ((x > 1) & (x < 1.5)).sum())\n        pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x < 2)).sum())\n        pauses_2_sec = group.apply(lambda x: ((x > 2) & (x < 3)).sum())\n        pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n        \n        result = pd.DataFrame({\n            'id': df['id'].unique(),\n            'largest_lantency': largest_lantency,\n            'smallest_lantency': smallest_lantency,\n            'median_lantency': median_lantency,\n            'initial_pause': initial_pause,\n            'pauses_half_sec': pauses_half_sec,\n            'pauses_1_sec': pauses_1_sec,\n            'pauses_1_half_sec': pauses_1_half_sec,\n            'pauses_2_sec': pauses_2_sec,\n            'pauses_3_sec': pauses_3_sec,\n        }).reset_index(drop=True)\n        return result\n    \n    def get_input_words(self, df):\n        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n        for percentile in self.percentiles:\n            tmp_df[f'input_word_length_pct_{percentile}'] = tmp_df['text_change'].apply(lambda x: np.percentile([len(i) for i in x] if len(x) > 0 else 0, \n                                                                                                               percentile))\n        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n        tmp_df.drop(['text_change'], axis=1, inplace=True)\n        return tmp_df\n    \n    def make_feats(self, df: pd.DataFrame, save_essays_path: str):\n        \n        print(\"Starting to engineer features\")\n        \n        # initialize features dataframe\n        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n        \n        # get essay feats\n        print(\"Getting essays\")\n        essay_df = self.essay_constructor.getEssays(df)\n        essay_df.to_csv(save_essays_path, index=False)\n\n        print(\"Getting essay aggregations data\")\n        essay_agg_df = self.get_essay_aggregations(essay_df)\n        feats = feats.merge(essay_agg_df, on='id', how='left')\n\n        print(\"Getting essay word aggregations data\")\n        word_df = self.split_essays_into_words(essay_df)\n        word_agg_df = self.compute_word_aggregations(word_df)\n        feats = feats.merge(word_agg_df, on='id', how='left')\n\n        print(\"Getting essay sentence aggregations data\")\n        sent_df = self.split_essays_into_sentences(essay_df)\n        sent_agg_df = self.compute_sentence_aggregations(sent_df)\n        feats = feats.merge(sent_agg_df, on='id', how='left')\n\n        print(\"Getting essay paragraph aggregations data\")\n        paragraph_df = self.split_essays_into_paragraphs(essay_df)\n        paragraph_agg_df = self.compute_paragraph_aggregations(paragraph_df)\n        feats = feats.merge(paragraph_agg_df, on='id', how='left')\n        \n        # engineer counts data\n        print(\"Engineering activity counts data\")\n        tmp_df = self.activity_counts(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n        \n        print(\"Engineering event counts data\")\n        tmp_df = self.event_counts(df, 'down_event')\n        feats = pd.concat([feats, tmp_df], axis=1)\n        tmp_df = self.event_counts(df, 'up_event')\n        feats = pd.concat([feats, tmp_df], axis=1)\n        \n        print(\"Engineering text change counts data\")\n        tmp_df = self.text_change_counts(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n        \n        print(\"Engineering punctuation counts data\")\n        tmp_df = self.match_punctuations(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n        \n        # space features\n        print(\"Engineering space-related data\")\n        tmp_df = self.make_space_features(df)\n        feats = feats.merge(tmp_df, on='id', how='left')\n        \n        # get shifted features\n        # time shift\n        print(\"Engineering time data\")\n        for gap in self.gaps:\n            print(f\"> for gap {gap}\")\n            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n        \n        # cursor position shift\n        print(\"Engineering cursor position data - gaps\")\n        for gap in self.gaps: \n            print(f\"> for gap {gap}\")\n            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n        \n        # word count shift\n        print(\"Engineering word count data - gaps\")\n        for gap in self.gaps: \n            print(f\"> for gap {gap}\")\n            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)\n        \n        # get aggregate statistical features\n        print(\"Engineering statistical summaries for features\")\n        # [(feature name, [ stat summaries to add ])]\n        percentiles_cols = [percentile(n) for n in self.percentiles]\n        feats_stat = [\n            ('event_id', ['max']),\n            ('up_time', ['first', 'last', 'max']),\n            ('down_time', ['first', 'last', 'max']),\n            ('action_time', ['max', 'mean', 'std', 'sem', 'skew', pd.DataFrame.kurt ] + self.percentiles_cols),\n            ('activity', ['nunique']),\n            ('down_event', [ 'nunique']),\n            ('up_event', [ 'nunique']),\n            ('text_change', [ 'nunique']),\n            ('cursor_position', ['max']),\n            ('word_count', ['max'])] \n\n        for gap in self.gaps:\n            feats_stat.extend([\n                (f'action_time_gap{gap}', ['first','last', 'max', 'min', 'mean', 'std', 'sem', 'skew', pd.DataFrame.kurt]+ percentiles_cols),\n                (f'cursor_position_change{gap}', ['first','last','max', 'mean', 'std','sem', 'skew', pd.DataFrame.kurt]),\n                (f'word_count_change{gap}', ['max', 'mean', 'std', 'sum', 'sem', 'skew', pd.DataFrame.kurt] + percentiles_cols),\n            ])\n        \n        pbar = tqdm(feats_stat)\n        for item in pbar:\n            colname, methods = item[0], item[1]\n            for method in methods:\n                pbar.set_postfix()\n                if isinstance(method, str):\n                    method_name = method\n                else:\n                    method_name = method.__name__\n                    \n                pbar.set_postfix(column=colname, method=method_name)\n                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n                feats = feats.merge(tmp_df, on='id', how='left') \n\n        print(\"Engineering input words data\")\n        tmp_df = self.get_input_words(df)\n        feats = pd.merge(feats, tmp_df, on='id', how='left')\n        \n        # compare feats\n        print(\"Engineering ratios data\")\n        feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n        feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n        feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n        feats['idle_time_ratio'] = feats['action_time_gap1_mean'] / feats['up_time_max']\n        \n        print(\"Done!\")\n        return feats","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:44:58.098477Z","iopub.execute_input":"2023-11-19T17:44:58.098812Z","iopub.status.idle":"2023-11-19T17:44:58.181648Z","shell.execute_reply.started":"2023-11-19T17:44:58.098782Z","shell.execute_reply":"2023-11-19T17:44:58.180794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Preprocessor","metadata":{}},{"cell_type":"code","source":"if CONFIG.use_pre_fe_data:\n    print(\"-\"*25)\n    print(\"Loading pre-engineered features for training data\")\n    print(\"-\"*25)\n    train_feats = pd.read_csv(CONFIG.pre_fe_data_filepath)\nelse:\n    preprocessor_train = Preprocessor(\n        seed = CONFIG.seed,\n    )\n    print(\"-\"*25)\n    print(\"Engineering features for training data\")\n    print(\"-\"*25)\n    train_feats = preprocessor_train.make_feats(\n        train_logs,\n        save_essays_path = 'train_essays.csv'\n    )\n    del preprocessor_train\n    gc.collect()\n\nprint()\nprint(\"-\"*25)\nprint(\"Engineering features for test data\")\nprint(\"-\"*25)\npreprocessor_test = Preprocessor(\n    seed = CONFIG.seed,\n)\ntest_feats = preprocessor_test.make_feats(\n    test_logs,\n    save_essays_path = 'test_essays.csv'\n)\ndel preprocessor_test\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:44:58.182702Z","iopub.execute_input":"2023-11-19T17:44:58.183002Z","iopub.status.idle":"2023-11-19T17:45:02.397277Z","shell.execute_reply.started":"2023-11-19T17:44:58.182979Z","shell.execute_reply":"2023-11-19T17:45:02.396396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_feats.to_csv('feat_eng_train_feats.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:45:02.39867Z","iopub.execute_input":"2023-11-19T17:45:02.399272Z","iopub.status.idle":"2023-11-19T17:45:04.013789Z","shell.execute_reply.started":"2023-11-19T17:45:02.399237Z","shell.execute_reply":"2023-11-19T17:45:04.012952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Shape of training data: {train_feats.shape}\")\nprint(f\"Shape of test data: {test_feats.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:45:04.015041Z","iopub.execute_input":"2023-11-19T17:45:04.015759Z","iopub.status.idle":"2023-11-19T17:45:04.020519Z","shell.execute_reply.started":"2023-11-19T17:45:04.015721Z","shell.execute_reply":"2023-11-19T17:45:04.019643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert train_feats.shape[1] == test_feats.shape[1], \"Train and test data must have same number of features.\"","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:45:04.021951Z","iopub.execute_input":"2023-11-19T17:45:04.022424Z","iopub.status.idle":"2023-11-19T17:45:04.031101Z","shell.execute_reply.started":"2023-11-19T17:45:04.022391Z","shell.execute_reply":"2023-11-19T17:45:04.030165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_feats.head()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-11-19T17:45:04.032209Z","iopub.execute_input":"2023-11-19T17:45:04.0325Z","iopub.status.idle":"2023-11-19T17:45:04.505131Z","shell.execute_reply.started":"2023-11-19T17:45:04.032475Z","shell.execute_reply":"2023-11-19T17:45:04.50417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_feats.head()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-11-19T17:45:04.50618Z","iopub.execute_input":"2023-11-19T17:45:04.506452Z","iopub.status.idle":"2023-11-19T17:45:04.8534Z","shell.execute_reply.started":"2023-11-19T17:45:04.506428Z","shell.execute_reply":"2023-11-19T17:45:04.852661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_feats = train_feats.merge(train_scores, on='id', how='left')","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:45:04.854561Z","iopub.execute_input":"2023-11-19T17:45:04.854854Z","iopub.status.idle":"2023-11-19T17:45:04.865937Z","shell.execute_reply.started":"2023-11-19T17:45:04.85483Z","shell.execute_reply":"2023-11-19T17:45:04.865026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split Data Into Folds","metadata":{}},{"cell_type":"code","source":"kfold = KFold(n_splits=CONFIG.num_folds, shuffle=True, random_state=CONFIG.seed)\n    \nfor fold, (_, val_idx) in enumerate(kfold.split(train_feats)):\n    train_feats.loc[val_idx, \"fold\"] = fold","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:45:04.867071Z","iopub.execute_input":"2023-11-19T17:45:04.867331Z","iopub.status.idle":"2023-11-19T17:45:04.880399Z","shell.execute_reply.started":"2023-11-19T17:45:04.867309Z","shell.execute_reply":"2023-11-19T17:45:04.879658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"markdown","source":"We encapsulate the model into a class, which allows us to\n- use the same class for different types of models (LGBM/XGB/Cat/Sklearn/etc)\n- cleans up the code, since we can just call one function to train/validate/predict","metadata":{}},{"cell_type":"code","source":"class WritingQualityModel:\n    \n    def __init__(self, model_name: str, params: dict):\n        self.model_name = model_name\n        self.model = self.create_model(model_name, params)\n    \n    def make_pipeline(self, model: str):\n        return Pipeline([\n            ('remove_infs', FunctionTransformer(lambda x: np.nan_to_num(x, nan=np.nan, posinf=0, neginf=0))),\n            ('imputer', SimpleImputer(strategy='mean')),\n            ('normalizer', FunctionTransformer(lambda x: np.log1p(np.abs(x)))),\n            ('scaler', RobustScaler()),\n            ('model', model)\n        ])\n    \n    def create_model(self, model_name: str, params: dict):\n        model = None\n        if 'lgbm' in model_name:\n            model = lgb.LGBMRegressor(**params)\n        elif 'cat' in model_name:\n            model = cb.CatBoostRegressor(**params)\n        elif 'rfr' in model_name:\n            model = RandomForestRegressor(**params)\n        elif 'lasso' in model_name:\n            model = self.make_pipeline(Lasso(**params))\n        elif 'ridge' in model_name:\n            model = self.make_pipeline(Ridge(**params))\n        return model\n\n    def train(self, X_train, Y_train, X_val, Y_val):\n        if any(x in self.model_name for x in ['lgbm']):\n            early_stopping_callback = lgb.early_stopping(CONFIG.num_trials_early_stopping, first_metric_only=True, verbose=False)\n\n            self.model.fit(X_train,\n                      Y_train,\n                      eval_set=[(X_val, Y_val)],\n                      eval_metric='rmse',\n                      verbose=0,\n                      callbacks=[early_stopping_callback])\n\n        elif any(x in self.model_name for x in ['cat']):\n            self.model.fit(X_train,\n                      Y_train,\n                      eval_set=[(X_val, Y_val)],\n                      verbose=0,\n                      early_stopping_rounds=CONFIG.num_trials_early_stopping)\n        else:\n            X_train = np.nan_to_num(X_train, posinf=-1, neginf=-1)\n            self.model.fit(X_train, Y_train)\n\n        return self.model\n    \n    def validate(self, X_val, Y_val):\n        if any(x in self.model_name for x in ['rfr', 'ridge', 'lasso']):\n            X_val = np.nan_to_num(X_val, posinf=-1, neginf=-1)\n            \n        pred = self.model.predict(X_val)\n        score = mean_squared_error(pred, Y_val, squared=False)\n        return pred, score\n    \n    def predict(self, X_test):\n        if any(x in self.model_name for x in ['rfr', 'ridge', 'lasso']):\n            X_test = np.nan_to_num(X_test, posinf=-1, neginf=-1)\n            \n        return self.model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:45:04.881671Z","iopub.execute_input":"2023-11-19T17:45:04.881924Z","iopub.status.idle":"2023-11-19T17:45:04.896949Z","shell.execute_reply.started":"2023-11-19T17:45:04.881902Z","shell.execute_reply":"2023-11-19T17:45:04.896036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_params_dict = {\n    'lgbm': {\n        'boosting_type': 'gbdt', \n        'metric': 'rmse',\n        'reg_alpha': 0.003188447814669599, \n        'reg_lambda': 0.0010228604507564066, \n        'colsample_bytree': 0.5420247656839267, \n        'subsample': 0.9778252382803456, \n        'feature_fraction': 0.8,\n        'bagging_freq': 1,\n        'bagging_fraction': 0.75,\n        'learning_rate': 0.01716485155812008, \n        'num_leaves': 19, \n        'min_child_samples': 46,\n        'verbosity': -1,\n        'random_state': 42,\n        'n_estimators': 500,\n    },\n    'cat': {\n        'learning_rate': 0.027407502438096695,\n        'min_child_samples': 23,\n        'iterations': 500,\n        'random_state': 419610,\n        'reg_lambda': 0.24381872974603785,\n        'subsample': 0.889148863756771,\n    },\n    'rfr': {\n        'max_depth': 6,\n        'max_features': 'sqrt',\n        'min_impurity_decrease': 0.0016295128631816343,\n        'n_estimators': 200,\n        'random_state': 42,\n    },\n    'ridge': {\n        'alpha': 1,\n        'random_state': 42,\n    },\n    'lasso': {\n        'alpha': 0.04198227921905038, \n        'max_iter': 2000, \n        'random_state': 42,\n    },\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:45:04.898095Z","iopub.execute_input":"2023-11-19T17:45:04.898394Z","iopub.status.idle":"2023-11-19T17:45:04.911291Z","shell.execute_reply.started":"2023-11-19T17:45:04.898363Z","shell.execute_reply":"2023-11-19T17:45:04.910571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Select Models","metadata":{}},{"cell_type":"code","source":"if CONFIG.model_names == 'all':\n    model_names = list(model_params_dict.keys())\nelse:\n    model_names = CONFIG.model_names","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:45:04.912359Z","iopub.execute_input":"2023-11-19T17:45:04.912677Z","iopub.status.idle":"2023-11-19T17:45:04.923413Z","shell.execute_reply.started":"2023-11-19T17:45:04.912648Z","shell.execute_reply":"2023-11-19T17:45:04.922668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Select Features","metadata":{}},{"cell_type":"code","source":"default_feature_names = list(\n        filter(lambda x: x not in [CONFIG.response_variate, 'id', 'fold'], train_feats.columns))\n\ndef get_features_for_model():\n    feature_names = default_feature_names\n    # filter out features with zero std deviation\n    feature_names = [name for name in feature_names if np.std(train_feats[name]) > 0]\n    return feature_names","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:45:04.924444Z","iopub.execute_input":"2023-11-19T17:45:04.924819Z","iopub.status.idle":"2023-11-19T17:45:04.934027Z","shell.execute_reply.started":"2023-11-19T17:45:04.924795Z","shell.execute_reply":"2023-11-19T17:45:04.933237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KFold Cross-Validation Trainer ","metadata":{}},{"cell_type":"markdown","source":"We also encapsulate the training, validation and prediction code into a Trainer class.","metadata":{}},{"cell_type":"code","source":"class KFoldTrainer:\n    \"\"\"Responsible for training a model using k-fold cross validation\"\"\"\n    \n    def __init__(self, seed: int, model_name: str, model_params: dict):\n        self.seed = seed\n        self.model_name = model_name\n        self.model_params = model_params\n        \n        # [fold]: model\n        self.saved_models = {}\n        \n    def train_by_fold(self, df: pd.DataFrame, feature_names: list[str], verbose: bool = False):\n        for fold in range(CONFIG.num_folds):\n            if verbose:\n                print(f\"Training for FOLD {fold}\")\n                \n            X_train = df[df[\"fold\"] != fold][feature_names]\n            Y_train = df[df[\"fold\"] != fold][CONFIG.response_variate]\n\n            X_val = df[df[\"fold\"] == fold][feature_names]\n            Y_val = df[df[\"fold\"] == fold][CONFIG.response_variate]\n\n            model = WritingQualityModel(self.model_name, self.model_params)\n            model.train(X_train.values, Y_train.values, X_val.values, Y_val.values)\n\n            self.saved_models[fold] = model\n    \n    def get_saved_models(self):\n        return self.saved_models\n    \n    # returns (df_pred, metric, fold_metrics, mean_fold_metric, sd_fold_metric)\n    def get_oof_predictions_and_metric(self, df: pd.DataFrame, feature_names: list[str], verbose: bool = False):\n        df_pred = pd.DataFrame({'index': df.index}).set_index('index')\n        fold_metrics = []\n        for fold in range(CONFIG.num_folds):\n            if verbose:\n                print(f\"Validating for FOLD {fold}\")\n                \n            X_val = df[df[\"fold\"] == fold][feature_names]\n            Y_val = df[df[\"fold\"] == fold][CONFIG.response_variate]\n            pred_fold = self.saved_models[fold].predict(X_val.values)\n            df_pred.loc[X_val.index, \"pred\"] = pred_fold\n            \n            fold_metric = mean_squared_error(Y_val.values, pred_fold, squared=False)\n            fold_metrics.append(fold_metric)\n            \n        metric = mean_squared_error(df[CONFIG.response_variate], df_pred[\"pred\"], squared=False)\n        mean_fold_metric, sd_fold_metric = np.mean(fold_metrics), np.std(fold_metrics)\n        return df_pred['pred'].values, metric, fold_metrics, mean_fold_metric, sd_fold_metric\n    \n    def predict(self, df: pd.DataFrame, feature_names: list[str], verbose: bool = False):\n        df_pred = pd.DataFrame({'index': df.index}).set_index('index')\n        for fold in range(CONFIG.num_folds):\n            if verbose:\n                print(f\"Predicting for FOLD {fold}\")\n            X_test = df[feature_names]\n            pred = self.saved_models[fold].predict(X_test.values)\n            df_pred[f\"pred{fold}\"] = pred\n            \n        df_pred[\"pred\"] = df_pred[[f\"pred{fold}\" for fold in range(CONFIG.num_folds)]].mean(axis=1)\n        return df_pred[\"pred\"].values","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:45:04.935216Z","iopub.execute_input":"2023-11-19T17:45:04.93547Z","iopub.status.idle":"2023-11-19T17:45:04.95131Z","shell.execute_reply.started":"2023-11-19T17:45:04.935448Z","shell.execute_reply":"2023-11-19T17:45:04.950378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training / Validation / Test Loop","metadata":{}},{"cell_type":"code","source":"print(f\"Out of a possible {len(default_feature_names)} features, we are using {len(get_features_for_model())} for training.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-19T17:45:04.952228Z","iopub.execute_input":"2023-11-19T17:45:04.952496Z","iopub.status.idle":"2023-11-19T17:45:05.09416Z","shell.execute_reply.started":"2023-11-19T17:45:04.952473Z","shell.execute_reply":"2023-11-19T17:45:05.093309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names = ['id'] + get_features_for_model()\ntrain_feats[feature_names].to_csv('feat_eng_train_feats_select_features.csv', index=False)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-11-19T17:45:05.095167Z","iopub.execute_input":"2023-11-19T17:45:05.09543Z","iopub.status.idle":"2023-11-19T17:45:06.675195Z","shell.execute_reply.started":"2023-11-19T17:45:05.095406Z","shell.execute_reply":"2023-11-19T17:45:06.673966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def postprocess_preds(preds):\n    # clipping\n    post_proc_preds = np.clip(preds, \n                              a_min=CONFIG.min_possible_response_value, \n                              a_max=CONFIG.max_possible_response_value)\n    \n    return post_proc_preds\n\nvalidation_rmses = {}\nmodel_scores = []\nmodel_fold_scores = []\nmodels_dict = {}\n\nfor idx, model_name in enumerate(model_names):\n    \n    print(\"=\"*25)\n    print(f\"Starting training, validation and prediction for model {model_name} [MODEL {idx+1}/{len(model_names)}]\")\n    print(\"=\"*25)\n    \n    print(\"-\"*25)\n    print(f\"Training model:\")\n    print(\"-\"*25)\n    \n    feature_names = get_features_for_model()\n    start_time = time.process_time()\n    trainer = KFoldTrainer(\n        seed = CONFIG.seed,\n        model_name = model_name,\n        model_params = model_params_dict[model_name]\n    )\n    # training\n    trainer.train_by_fold(train_feats, feature_names, verbose=True)\n    print(\"-\"*25)\n    \n    models_dict[model_name] = trainer.get_saved_models()\n    \n    # validation\n    pred_train, metric, fold_metrics, mean_fold_metric, sd_fold_metric = trainer.get_oof_predictions_and_metric(train_feats, feature_names)\n    train_feats[f'pred_{CONFIG.response_variate}_{model_name}'] = postprocess_preds(pred_train)\n    \n    for fold in range(CONFIG.num_folds):\n        print(f\"RMSE for FOLD {fold}: {fold_metrics[fold]:6f}\")\n    print()\n    print(f\"OOF RMSE for {model_name}: {metric:.6f}\")\n    print(f\"Mean/SD RMSE for {model_name}: {mean_fold_metric:.6f}  {sd_fold_metric:.6f}\")\n    \n    # for plotting later on\n    validation_rmses[model_name] = metric \n    model_scores.append({\n        'model_name': model_name,\n        'score': metric,\n    })\n    model_fold_scores.extend([{\n        'model_name': model_name,\n        'fold': fold,\n        'score': fold_metrics[fold]\n    } for fold in range(CONFIG.num_folds)])\n    \n    # prediction (test set)\n    print(\"-\"*25)\n    print(f\"Predicting test set with model:\")\n    print(\"-\"*25)\n    test_feats[f'pred_{CONFIG.response_variate}_{model_name}'] = postprocess_preds(\n        trainer.predict(test_feats, feature_names, verbose=True))\n    \n    # cleanup\n    del trainer, pred_train, metric, fold_metrics, mean_fold_metric, sd_fold_metric\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:45:06.67716Z","iopub.execute_input":"2023-11-19T17:45:06.677559Z","iopub.status.idle":"2023-11-19T17:48:31.537525Z","shell.execute_reply.started":"2023-11-19T17:45:06.677527Z","shell.execute_reply":"2023-11-19T17:48:31.536648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Add LightAutoML Predictions","metadata":{}},{"cell_type":"code","source":"import joblib\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML\nfrom lightautoml.tasks import Task\n\nlight_automl_model = joblib.load(CONFIG.lightautoml_model_path)\nlight_automl_oof_preds = joblib.load(CONFIG.lightautoml_oof_preds_path)\n\ntrain_feats[f'pred_{CONFIG.response_variate}_lightautoml'] = light_automl_oof_preds.data.reshape(-1)\ntest_feats[f'pred_{CONFIG.response_variate}_lightautoml'] = light_automl_model.predict(test_feats[get_features_for_model()]).data\n\nlightautoml_rmse = mean_squared_error(train_feats[f'pred_{CONFIG.response_variate}_lightautoml'], train_feats[CONFIG.response_variate], squared=False)\nvalidation_rmses['lightautoml'] = lightautoml_rmse\nprint(f\"OOF RMSE for LightAutoML RMSE: {lightautoml_rmse}\")\n\nmodel_names.append('lightautoml')\nmodel_scores.append({ 'model_name': 'lightautoml', 'score': lightautoml_rmse })","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:49:31.812063Z","iopub.execute_input":"2023-11-19T17:49:31.812441Z","iopub.status.idle":"2023-11-19T17:49:53.378238Z","shell.execute_reply.started":"2023-11-19T17:49:31.812413Z","shell.execute_reply":"2023-11-19T17:49:53.376988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mean Feature Importances Of (Decision Tree) Models","metadata":{}},{"cell_type":"code","source":"model_decision_trees = set(['lgbm', 'cat', 'rfr']).intersection(set(model_names))","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:50:20.12547Z","iopub.execute_input":"2023-11-19T17:50:20.12637Z","iopub.status.idle":"2023-11-19T17:50:20.130914Z","shell.execute_reply.started":"2023-11-19T17:50:20.126334Z","shell.execute_reply":"2023-11-19T17:50:20.129912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_name: feat_df\nmodel_feat_dfs = {}\n\nall_feature_importances_df = pd.DataFrame({'name': feature_names})\n\nfor model_name in model_decision_trees:\n    fold_models = models_dict[model_name].values()\n    \n    feature_importances_values = np.asarray([model.model.feature_importances_ for model in fold_models]).mean(axis=0)\n    feature_importance_df = pd.DataFrame({'name': feature_names, 'importance': feature_importances_values})\n    all_feature_importances_df = all_feature_importances_df.merge(\n        feature_importance_df.rename(columns={'importance': f'importance_{model_name}'}), \n        on='name', \n        suffixes=(None, None))\n\n    feature_importance_df = feature_importance_df.sort_values('importance', ascending=False).head(CONFIG.num_features_to_display)\n    model_feat_dfs[model_name] = feature_importance_df\n    \nfig, axes = plt.subplots(nrows=len(model_decision_trees), \n                         figsize=(12, len(model_decision_trees)*8))\nplt.subplots_adjust(hspace=1)\n\nif len(model_decision_trees) == 1:\n    axes = [axes]\n\nfor ax, model_name in zip(axes, model_decision_trees):\n    \n    feature_importance_df = model_feat_dfs[model_name]\n    \n    sns.barplot(data=feature_importance_df, x='name', y='importance', ax=ax)\n    ax.set_title(f\"Mean feature importances for model {model_name}\")\n    ax.set_xticks(ax.get_xticks(), ax.get_xticklabels(), rotation=90)\n\nplt.show()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-11-19T17:50:21.187409Z","iopub.execute_input":"2023-11-19T17:50:21.187806Z","iopub.status.idle":"2023-11-19T17:50:23.239842Z","shell.execute_reply.started":"2023-11-19T17:50:21.187773Z","shell.execute_reply":"2023-11-19T17:50:23.238839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that our new essay features have good feature importance.","metadata":{}},{"cell_type":"markdown","source":"# Visualizations of Model Performance","metadata":{}},{"cell_type":"markdown","source":"## Graph of OOF RMSEs","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(len(model_names)*1, 8))\n\npred_df = pd.DataFrame.from_records(model_scores)\nmin_score = pred_df['score'].min()\nmax_score = pred_df['score'].max()\n\norder = pred_df.sort_values('score', ascending=True)['model_name']\nax = sns.barplot(data=pred_df, x='model_name', y='score', order=order)\nax.bar_label(ax.containers[0], fmt='{:,.5f}')\nax.set(ylim=(min_score-0.05, max_score+0.05))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:50:53.733667Z","iopub.execute_input":"2023-11-19T17:50:53.734508Z","iopub.status.idle":"2023-11-19T17:50:53.957686Z","shell.execute_reply.started":"2023-11-19T17:50:53.734475Z","shell.execute_reply":"2023-11-19T17:50:53.956821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation Matrix of Model OOF Predictions and Score\n\nReasoning behind this: highly correlated models do not ensemble as well as (relatively) lowly correlated models.","metadata":{}},{"cell_type":"code","source":"pred_and_score_cols = [f\"pred_{CONFIG.response_variate}_{model_name}\" for model_name in model_names] + [CONFIG.response_variate]\n\nplt.figure(figsize=(10, 10))\nsns.heatmap(train_feats[pred_and_score_cols].corr(numeric_only=True), annot=True, fmt='.4f')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:52:09.336528Z","iopub.execute_input":"2023-11-19T17:52:09.337166Z","iopub.status.idle":"2023-11-19T17:52:09.780172Z","shell.execute_reply.started":"2023-11-19T17:52:09.337129Z","shell.execute_reply":"2023-11-19T17:52:09.779283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Boxplots of Fold RMSEs\n\nReasoning: this can help investigate the spread of the RMSEs for each fold.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 8))\n\nfold_pred_df = pd.DataFrame.from_records(model_fold_scores)\norder = fold_pred_df.groupby('model_name').median().sort_values('score', ascending=True).index\nax = sns.boxplot(data=fold_pred_df, x='model_name', y='score', order=order)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:52:19.175054Z","iopub.execute_input":"2023-11-19T17:52:19.175763Z","iopub.status.idle":"2023-11-19T17:52:19.491298Z","shell.execute_reply.started":"2023-11-19T17:52:19.175728Z","shell.execute_reply":"2023-11-19T17:52:19.490395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Boxenplots of Residuals For Each Model\n\nReasoning: this can help investigate where our models perform worse. In particular, we see that our models perform worse in the extremes.","metadata":{}},{"cell_type":"code","source":"# make score into category so we can use it to categorize residuals\ntrain_feats['score_cat'] = train_feats['score'].astype('category')\n# make residuals\nfor model_name in model_names:\n    train_feats[f'diff_{model_name}'] = train_feats[f'pred_{CONFIG.response_variate}_{model_name}'] - train_feats[CONFIG.response_variate]\ndiff_cols = [f'diff_{model_name}' for model_name in model_names]\n    \nfig, axes = plt.subplots(nrows=len(model_names), figsize=(15, len(model_names)*6))\n\nfor idx, col in enumerate(diff_cols): \n    sns.boxenplot(data=train_feats, y=col, ax=axes[idx], x='score_cat')\n    axes[idx].set_title(f'{col} by score')\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:53:19.445829Z","iopub.execute_input":"2023-11-19T17:53:19.446223Z","iopub.status.idle":"2023-11-19T17:53:21.533017Z","shell.execute_reply.started":"2023-11-19T17:53:19.446191Z","shell.execute_reply":"2023-11-19T17:53:21.532091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensembling Using Hill Climbing","metadata":{}},{"cell_type":"code","source":"# [(model_name, weight)]\nmodel_weights = {}\n# preds\nbest_ensemble_train_preds = None\nbest_ensemble_test_preds = None\n\nprint(\"-\"*25)\nprint(f\"Running hill climbing\")\nprint(\"-\"*25)\n\n# Initialise\nSTOP = False\n# [model_name]: model_weight\nmodel_weights = {}\nbest_model_order = [a[0] for a in sorted(validation_rmses.items(), key=lambda x: x[1])]\ni = 0\n\ncur_model_names = list(set(model_names).difference(set([best_model_order[0]]))).copy()\ny_target = train_feats[CONFIG.response_variate].values\n\nbest_ensemble_train_preds = train_feats[f\"pred_{CONFIG.response_variate}_{best_model_order[0]}\"]\nbest_ensemble_test_preds = test_feats[f\"pred_{CONFIG.response_variate}_{best_model_order[0]}\"]\n\npotential_new_best_cv_score = mean_squared_error(y_target, best_ensemble_train_preds, squared=False)\nmodel_weights[best_model_order[0]] = 1\nprint(f\"Initial best single model RMSE ({best_model_order[0]}): {potential_new_best_cv_score}\")\n\nweight_range = np.arange(-0.6, 0.6, 0.001)\n\n# Hill climbing\nwhile not STOP:\n    i += 1\n    potential_new_best_cv_score = mean_squared_error(y_target, best_ensemble_train_preds, squared=False)\n    k_best, model_name_best, wgt_best = None, None, None\n    for k, model_name in enumerate(cur_model_names):\n        for wgt in weight_range:\n            potential_ensemble = (1-wgt) * best_ensemble_train_preds + wgt * train_feats[f\"pred_{CONFIG.response_variate}_{model_name}\"]\n            cv_score = mean_squared_error(y_target, potential_ensemble, squared=False)\n            if cv_score < potential_new_best_cv_score:\n                potential_new_best_cv_score = cv_score\n                k_best, model_name_best, wgt_best = k, model_name, wgt\n\n    if k_best is not None:\n        best_ensemble_train_preds = (1-wgt_best) * best_ensemble_train_preds + wgt_best * train_feats[f\"pred_{CONFIG.response_variate}_{model_name_best}\"]\n        best_ensemble_test_preds = (1-wgt_best) * best_ensemble_test_preds + wgt_best * test_feats[f\"pred_{CONFIG.response_variate}_{model_name_best}\"]\n\n        model_weights = {model_name: model_weight * (1-wgt_best) for model_name, model_weight in model_weights.items()}\n        model_weights[model_name_best] = wgt_best\n\n        cur_model_names.remove(model_name_best)\n        if len(cur_model_names) == 0:\n            STOP = True\n        print(f'Iteration: {i}, Model added: {model_name_best}, Best weight: {wgt_best:.5f}, Best RMSE: {potential_new_best_cv_score:.7f}')\n    else:\n        STOP = True","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:53:32.705343Z","iopub.execute_input":"2023-11-19T17:53:32.70599Z","iopub.status.idle":"2023-11-19T17:53:41.88302Z","shell.execute_reply.started":"2023-11-19T17:53:32.705957Z","shell.execute_reply":"2023-11-19T17:53:41.882065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Weights","metadata":{}},{"cell_type":"code","source":"model_weights_df = pd.DataFrame(model_weights.items(), columns=['model_name', 'weight'])\nmodel_weights_df.sort_values('weight', ascending=False, inplace=True)\n\nplt.figure(figsize=(18, 6))\nax = sns.barplot(data=model_weights_df, x='model_name', y='weight')\nax.bar_label(ax.containers[0], fmt='{:,.3f}')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:53:46.227165Z","iopub.execute_input":"2023-11-19T17:53:46.227547Z","iopub.status.idle":"2023-11-19T17:53:46.542709Z","shell.execute_reply.started":"2023-11-19T17:53:46.227518Z","shell.execute_reply":"2023-11-19T17:53:46.541614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict Train and Test Set with Ensemble Predictions","metadata":{}},{"cell_type":"code","source":"train_feats[f\"pred_{CONFIG.response_variate}\"] = best_ensemble_train_preds\ntest_feats[CONFIG.response_variate] = best_ensemble_test_preds","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:53:49.049684Z","iopub.execute_input":"2023-11-19T17:53:49.050485Z","iopub.status.idle":"2023-11-19T17:53:49.057385Z","shell.execute_reply.started":"2023-11-19T17:53:49.050442Z","shell.execute_reply":"2023-11-19T17:53:49.056244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scatter Plot of Predictions vs Actual","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6, 6))\nax = sns.scatterplot(data=train_feats, x=CONFIG.response_variate, y=f\"pred_{CONFIG.response_variate}\")\nsns.lineplot(data=train_feats, x=CONFIG.response_variate, y=CONFIG.response_variate, ax=ax, color=\"black\")\nax.set_title(\"Predicted vs Actual\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:53:51.758276Z","iopub.execute_input":"2023-11-19T17:53:51.758865Z","iopub.status.idle":"2023-11-19T17:53:52.608516Z","shell.execute_reply.started":"2023-11-19T17:53:51.758831Z","shell.execute_reply":"2023-11-19T17:53:52.607564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"submission = test_feats[[\"id\", CONFIG.response_variate]]\nsubmission","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:53:59.709377Z","iopub.execute_input":"2023-11-19T17:53:59.710101Z","iopub.status.idle":"2023-11-19T17:53:59.720691Z","shell.execute_reply.started":"2023-11-19T17:53:59.710063Z","shell.execute_reply":"2023-11-19T17:53:59.719684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T17:54:01.38455Z","iopub.execute_input":"2023-11-19T17:54:01.384909Z","iopub.status.idle":"2023-11-19T17:54:01.391518Z","shell.execute_reply.started":"2023-11-19T17:54:01.384882Z","shell.execute_reply":"2023-11-19T17:54:01.390498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Thanks for reading! Make sure to upvote if you forked/liked this!","metadata":{}}]}