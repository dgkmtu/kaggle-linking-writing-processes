{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":59291,"databundleVersionId":6678907,"sourceType":"competition"},{"sourceId":6971449,"sourceType":"datasetVersion","datasetId":3992884},{"sourceId":6973319,"sourceType":"datasetVersion","datasetId":3949123},{"sourceId":150384981,"sourceType":"kernelVersion"}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --no-index -U --find-links=/kaggle/input/lightautoml-038-dependecies lightautoml==0.3.8\n!pip install --no-index -U --find-links=/kaggle/input/lightautoml-038-dependecies pandas==2.0.3","metadata":{"execution":{"iopub.status.busy":"2023-11-20T12:28:09.506345Z","iopub.execute_input":"2023-11-20T12:28:09.507371Z","iopub.status.idle":"2023-11-20T12:29:07.197349Z","shell.execute_reply.started":"2023-11-20T12:28:09.507312Z","shell.execute_reply":"2023-11-20T12:29:07.195646Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Looking in links: /kaggle/input/lightautoml-038-dependecies\nProcessing /kaggle/input/lightautoml-038-dependecies/lightautoml-0.3.8-py3-none-any.whl\nProcessing /kaggle/input/lightautoml-038-dependecies/AutoWoE-1.3.2-py3-none-any.whl (from lightautoml==0.3.8)\nRequirement already satisfied: catboost>=0.26.1 in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (1.2.2)\nProcessing /kaggle/input/lightautoml-038-dependecies/cmaes-0.10.0-py3-none-any.whl (from lightautoml==0.3.8)\nRequirement already satisfied: holidays in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (0.24)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (3.1.2)\nProcessing /kaggle/input/lightautoml-038-dependecies/joblib-1.2.0-py3-none-any.whl (from lightautoml==0.3.8)\nProcessing /kaggle/input/lightautoml-038-dependecies/json2html-1.3.0.tar.gz (from lightautoml==0.3.8)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hProcessing /kaggle/input/lightautoml-038-dependecies/lightgbm-3.2.1-py3-none-manylinux1_x86_64.whl (from lightautoml==0.3.8)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (3.1)\nRequirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (1.24.3)\nRequirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (3.4.0)\nProcessing /kaggle/input/lightautoml-038-dependecies/pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from lightautoml==0.3.8)\nProcessing /kaggle/input/lightautoml-038-dependecies/poetry_core-1.8.1-py3-none-any.whl (from lightautoml==0.3.8)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (6.0.1)\nRequirement already satisfied: scikit-learn>=0.22 in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (1.2.2)\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (0.12.2)\nRequirement already satisfied: statsmodels<=0.14.0 in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (0.14.0)\nRequirement already satisfied: torch<=2.0.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (2.0.0+cpu)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (4.66.1)\nProcessing /kaggle/input/lightautoml-038-dependecies/StrEnum-0.4.15-py3-none-any.whl (from autowoe>=1.2->lightautoml==0.3.8)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from autowoe>=1.2->lightautoml==0.3.8) (3.7.3)\nRequirement already satisfied: pytest in /opt/conda/lib/python3.10/site-packages (from autowoe>=1.2->lightautoml==0.3.8) (7.4.3)\nRequirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from autowoe>=1.2->lightautoml==0.3.8) (2023.3)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from autowoe>=1.2->lightautoml==0.3.8) (1.11.3)\nProcessing /kaggle/input/lightautoml-038-dependecies/sphinx-7.2.6-py3-none-any.whl (from autowoe>=1.2->lightautoml==0.3.8)\nRequirement already satisfied: sphinx-rtd-theme in /opt/conda/lib/python3.10/site-packages (from autowoe>=1.2->lightautoml==0.3.8) (0.2.4)\nRequirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from catboost>=0.26.1->lightautoml==0.3.8) (0.20.1)\nRequirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from catboost>=0.26.1->lightautoml==0.3.8) (5.16.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from catboost>=0.26.1->lightautoml==0.3.8) (1.16.0)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from lightgbm<=3.2.1,>=2.3->lightautoml==0.3.8) (0.41.2)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.0.0->lightautoml==0.3.8) (2.8.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22->lightautoml==0.3.8) (3.2.0)\nRequirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.10/site-packages (from statsmodels<=0.14.0->lightautoml==0.3.8) (0.5.3)\nRequirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from statsmodels<=0.14.0->lightautoml==0.3.8) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<=2.0.0,>=1.9.0->lightautoml==0.3.8) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch<=2.0.0,>=1.9.0->lightautoml==0.3.8) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<=2.0.0,>=1.9.0->lightautoml==0.3.8) (1.12)\nRequirement already satisfied: hijri-converter in /opt/conda/lib/python3.10/site-packages (from holidays->lightautoml==0.3.8) (2.3.1)\nRequirement already satisfied: korean-lunar-calendar in /opt/conda/lib/python3.10/site-packages (from holidays->lightautoml==0.3.8) (0.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->lightautoml==0.3.8) (2.1.3)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna->lightautoml==0.3.8) (1.12.1)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna->lightautoml==0.3.8) (6.7.0)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna->lightautoml==0.3.8) (2.0.20)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->lightautoml==0.3.8) (1.3.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autowoe>=1.2->lightautoml==0.3.8) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autowoe>=1.2->lightautoml==0.3.8) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autowoe>=1.2->lightautoml==0.3.8) (4.42.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autowoe>=1.2->lightautoml==0.3.8) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autowoe>=1.2->lightautoml==0.3.8) (10.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autowoe>=1.2->lightautoml==0.3.8) (3.0.9)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna->lightautoml==0.3.8) (2.0.2)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly->catboost>=0.26.1->lightautoml==0.3.8) (8.2.3)\nRequirement already satisfied: iniconfig in /opt/conda/lib/python3.10/site-packages (from pytest->autowoe>=1.2->lightautoml==0.3.8) (2.0.0)\nRequirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from pytest->autowoe>=1.2->lightautoml==0.3.8) (1.2.0)\nRequirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/lib/python3.10/site-packages (from pytest->autowoe>=1.2->lightautoml==0.3.8) (1.1.3)\nRequirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from pytest->autowoe>=1.2->lightautoml==0.3.8) (2.0.1)\nProcessing /kaggle/input/lightautoml-038-dependecies/sphinxcontrib_applehelp-1.0.7-py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\nProcessing /kaggle/input/lightautoml-038-dependecies/sphinxcontrib_devhelp-1.0.5-py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\nProcessing /kaggle/input/lightautoml-038-dependecies/sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\nProcessing /kaggle/input/lightautoml-038-dependecies/sphinxcontrib_htmlhelp-2.0.4-py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\nProcessing /kaggle/input/lightautoml-038-dependecies/sphinxcontrib_serializinghtml-1.1.9-py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\nProcessing /kaggle/input/lightautoml-038-dependecies/sphinxcontrib_qthelp-1.0.6-py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\nRequirement already satisfied: Pygments>=2.14 in /opt/conda/lib/python3.10/site-packages (from sphinx->autowoe>=1.2->lightautoml==0.3.8) (2.16.1)\nRequirement already satisfied: docutils<0.21,>=0.18.1 in /opt/conda/lib/python3.10/site-packages (from sphinx->autowoe>=1.2->lightautoml==0.3.8) (0.20.1)\nRequirement already satisfied: snowballstemmer>=2.0 in /opt/conda/lib/python3.10/site-packages (from sphinx->autowoe>=1.2->lightautoml==0.3.8) (2.2.0)\nRequirement already satisfied: babel>=2.9 in /opt/conda/lib/python3.10/site-packages (from sphinx->autowoe>=1.2->lightautoml==0.3.8) (2.12.1)\nProcessing /kaggle/input/lightautoml-038-dependecies/alabaster-0.7.13-py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\nProcessing /kaggle/input/lightautoml-038-dependecies/imagesize-1.4.1-py2.py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\nRequirement already satisfied: requests>=2.25.0 in /opt/conda/lib/python3.10/site-packages (from sphinx->autowoe>=1.2->lightautoml==0.3.8) (2.31.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<=2.0.0,>=1.9.0->lightautoml==0.3.8) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx->autowoe>=1.2->lightautoml==0.3.8) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx->autowoe>=1.2->lightautoml==0.3.8) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx->autowoe>=1.2->lightautoml==0.3.8) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx->autowoe>=1.2->lightautoml==0.3.8) (2023.7.22)\nBuilding wheels for collected packages: json2html\n  Building wheel for json2html (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for json2html: filename=json2html-1.3.0-py3-none-any.whl size=7591 sha256=f4639234561c14a92bfb3b59e08521a6eb1fb662b5f38e363795d4d1107204bc\n  Stored in directory: /root/.cache/pip/wheels/03/04/0d/34912ecabd9128a537a032c0fc15c6c46e734fb5fe3a14536c\nSuccessfully built json2html\nInstalling collected packages: StrEnum, json2html, sphinxcontrib-jsmath, poetry-core, joblib, imagesize, cmaes, alabaster, pandas, lightgbm, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, sphinx, autowoe, lightautoml\n  Attempting uninstall: joblib\n    Found existing installation: joblib 1.3.2\n    Uninstalling joblib-1.3.2:\n      Successfully uninstalled joblib-1.3.2\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.0.3\n    Uninstalling pandas-2.0.3:\n      Successfully uninstalled pandas-2.0.3\n  Attempting uninstall: lightgbm\n    Found existing installation: lightgbm 3.3.2\n    Uninstalling lightgbm-3.3.2:\n      Successfully uninstalled lightgbm-3.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbeatrix-jupyterlab 2023.814.150030 requires jupyter-server~=1.16, but you have jupyter-server 2.10.0 which is incompatible.\nbeatrix-jupyterlab 2023.814.150030 requires jupyterlab~=3.4, but you have jupyterlab 4.0.5 which is incompatible.\nfitter 1.6.0 requires joblib<2.0.0,>=1.3.1, but you have joblib 1.2.0 which is incompatible.\nfitter 1.6.0 requires pandas<3.0.0,>=2.0.3, but you have pandas 1.5.3 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.3 which is incompatible.\ntensorflowjs 4.13.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed StrEnum-0.4.15 alabaster-0.7.13 autowoe-1.3.2 cmaes-0.10.0 imagesize-1.4.1 joblib-1.2.0 json2html-1.3.0 lightautoml-0.3.8 lightgbm-3.2.1 pandas-1.5.3 poetry-core-1.8.1 sphinx-7.2.6 sphinxcontrib-applehelp-1.0.7 sphinxcontrib-devhelp-1.0.5 sphinxcontrib-htmlhelp-2.0.4 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.6 sphinxcontrib-serializinghtml-1.1.9\nLooking in links: /kaggle/input/lightautoml-038-dependecies\nProcessing /kaggle/input/lightautoml-038-dependecies/pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas==2.0.3) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas==2.0.3) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas==2.0.3) (2023.3)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas==2.0.3) (1.24.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.16.0)\nInstalling collected packages: pandas\n  Attempting uninstall: pandas\n    Found existing installation: pandas 1.5.3\n    Uninstalling pandas-1.5.3:\n      Successfully uninstalled pandas-1.5.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbeatrix-jupyterlab 2023.814.150030 requires jupyter-server~=1.16, but you have jupyter-server 2.10.0 which is incompatible.\nbeatrix-jupyterlab 2023.814.150030 requires jupyterlab~=3.4, but you have jupyterlab 4.0.5 which is incompatible.\nfitter 1.6.0 requires joblib<2.0.0,>=1.3.1, but you have joblib 1.2.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nlightautoml 0.3.8 requires pandas<2.0.0, but you have pandas 2.0.3 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.3 which is incompatible.\ntensorflowjs 4.13.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pandas-2.0.3\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Set Global Configuration Options","metadata":{}},{"cell_type":"code","source":"class CONFIG:\n    '''\n    > General Options\n    '''\n    # global seed\n    seed = 42\n    # the number of samples to use for testing purposes\n    # if None, we use the full dataset\n    samples_testing = None #None\n    # max rows to display for pandas dataframes\n    display_max_rows = 200\n    # name of the response variate we are trying to predict\n    response_variate = 'score'\n    # minimum value for response variate\n    min_possible_response_value = 0.5\n    # maximum value for response variate\n    max_possible_response_value = 6.0\n    \n    '''\n    > Feature Engineering Options\n    '''\n    # whether to use pre feature engineered data or not\n    use_pre_fe_data = True\n    # fe data saved path\n    pre_fe_data_filepath = '/kaggle/input/writing-quality-baseline-v2-train-data/feat_eng_train_feats.csv'\n    \n    '''\n    > Preprocessing Options\n    '''\n    # number of folds to split the data for CV\n    num_folds = 10\n    \n    '''\n    > Modelling + Training Options\n    '''\n    # the names of the models to use\n    # either a list of model names, or 'all', in which case all models are used\n    model_names = 'all'\n    # number of trials to use for early stopping\n    num_trials_early_stopping = 50\n    # model path for lightautoml\n    lightautoml_model_path = '/kaggle/input/writing-quality-baseline-v2-lightautoml/denselight.model'\n    # oof preds path for lightautoml\n    lightautoml_oof_preds_path = '/kaggle/input/writing-quality-baseline-v2-lightautoml/denselight_oof_preds'\n    \n    '''\n    > Post-Modelling Options\n    '''\n    # number of most important features to display\n    # for feature importances plots\n    num_features_to_display = 50","metadata":{"execution":{"iopub.status.busy":"2023-11-20T12:29:07.200536Z","iopub.execute_input":"2023-11-20T12:29:07.201200Z","iopub.status.idle":"2023-11-20T12:29:07.214574Z","shell.execute_reply.started":"2023-11-20T12:29:07.201133Z","shell.execute_reply":"2023-11-20T12:29:07.212719Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import warnings\n\nimport os\nimport gc\nimport re\nimport random\nfrom collections import Counter, defaultdict\nimport pprint\nimport pickle\nimport time\nimport copy\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.autonotebook import tqdm\n\n# from gensim.models import Word2Vec\nfrom sklearn.preprocessing import LabelEncoder, PowerTransformer, RobustScaler, FunctionTransformer\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD, PCA\nfrom sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cluster import KMeans\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.impute import SimpleImputer\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nimport optuna","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-20T12:29:07.217513Z","iopub.execute_input":"2023-11-20T12:29:07.218013Z","iopub.status.idle":"2023-11-20T12:29:13.197618Z","shell.execute_reply.started":"2023-11-20T12:29:07.217963Z","shell.execute_reply":"2023-11-20T12:29:13.195964Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/150713335.py:17: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Set Some Options","metadata":{}},{"cell_type":"code","source":"tqdm.pandas()\nsns.set_style(\"whitegrid\")\n\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.max_rows\", CONFIG.display_max_rows)\nwarnings.simplefilter('ignore')\n\nrandom.seed(CONFIG.seed)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T12:32:14.115983Z","iopub.execute_input":"2023-11-20T12:32:14.116397Z","iopub.status.idle":"2023-11-20T12:32:14.124509Z","shell.execute_reply.started":"2023-11-20T12:32:14.116365Z","shell.execute_reply":"2023-11-20T12:32:14.123134Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"%%time\nINPUT_DIR = '/kaggle/input/linking-writing-processes-to-writing-quality'\ntrain_logs = pd.read_csv(f'{INPUT_DIR}/train_logs.csv')\ntrain_scores = pd.read_csv(f'{INPUT_DIR}/train_scores.csv')\ntest_logs = pd.read_csv(f'{INPUT_DIR}/test_logs.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-20T12:32:49.629245Z","iopub.execute_input":"2023-11-20T12:32:49.629705Z","iopub.status.idle":"2023-11-20T12:33:09.429203Z","shell.execute_reply.started":"2023-11-20T12:32:49.629671Z","shell.execute_reply":"2023-11-20T12:33:09.427987Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"CPU times: user 11.8 s, sys: 3.12 s, total: 14.9 s\nWall time: 19.8 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Subsample Data (If Specified)","metadata":{}},{"cell_type":"code","source":"if CONFIG.samples_testing is not None:\n    ids = list(train_logs[\"id\"].unique())\n    sample_ids = random.sample(ids, CONFIG.samples_testing)\n    train_logs = train_logs[train_logs[\"id\"].isin(sample_ids)]","metadata":{"execution":{"iopub.status.busy":"2023-11-20T12:34:54.282129Z","iopub.execute_input":"2023-11-20T12:34:54.282715Z","iopub.status.idle":"2023-11-20T12:34:54.291200Z","shell.execute_reply.started":"2023-11-20T12:34:54.282673Z","shell.execute_reply":"2023-11-20T12:34:54.289202Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Looking At Data","metadata":{}},{"cell_type":"code","source":"train_logs.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T12:35:31.061579Z","iopub.execute_input":"2023-11-20T12:35:31.062181Z","iopub.status.idle":"2023-11-20T12:35:31.097147Z","shell.execute_reply.started":"2023-11-20T12:35:31.062132Z","shell.execute_reply":"2023-11-20T12:35:31.095937Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"         id  event_id  down_time  up_time  action_time       activity  \\\n0  001519c8         1       4526     4557           31  Nonproduction   \n1  001519c8         2       4558     4962          404  Nonproduction   \n2  001519c8         3     106571   106571            0  Nonproduction   \n3  001519c8         4     106686   106777           91          Input   \n4  001519c8         5     107196   107323          127          Input   \n\n  down_event   up_event text_change  cursor_position  word_count  \n0  Leftclick  Leftclick    NoChange                0           0  \n1  Leftclick  Leftclick    NoChange                0           0  \n2      Shift      Shift    NoChange                0           0  \n3          q          q           q                1           1  \n4          q          q           q                2           1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>event_id</th>\n      <th>down_time</th>\n      <th>up_time</th>\n      <th>action_time</th>\n      <th>activity</th>\n      <th>down_event</th>\n      <th>up_event</th>\n      <th>text_change</th>\n      <th>cursor_position</th>\n      <th>word_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>001519c8</td>\n      <td>1</td>\n      <td>4526</td>\n      <td>4557</td>\n      <td>31</td>\n      <td>Nonproduction</td>\n      <td>Leftclick</td>\n      <td>Leftclick</td>\n      <td>NoChange</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>001519c8</td>\n      <td>2</td>\n      <td>4558</td>\n      <td>4962</td>\n      <td>404</td>\n      <td>Nonproduction</td>\n      <td>Leftclick</td>\n      <td>Leftclick</td>\n      <td>NoChange</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>001519c8</td>\n      <td>3</td>\n      <td>106571</td>\n      <td>106571</td>\n      <td>0</td>\n      <td>Nonproduction</td>\n      <td>Shift</td>\n      <td>Shift</td>\n      <td>NoChange</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>001519c8</td>\n      <td>4</td>\n      <td>106686</td>\n      <td>106777</td>\n      <td>91</td>\n      <td>Input</td>\n      <td>q</td>\n      <td>q</td>\n      <td>q</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>001519c8</td>\n      <td>5</td>\n      <td>107196</td>\n      <td>107323</td>\n      <td>127</td>\n      <td>Input</td>\n      <td>q</td>\n      <td>q</td>\n      <td>q</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_scores.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T12:35:40.485026Z","iopub.execute_input":"2023-11-20T12:35:40.485558Z","iopub.status.idle":"2023-11-20T12:35:40.500981Z","shell.execute_reply.started":"2023-11-20T12:35:40.485517Z","shell.execute_reply":"2023-11-20T12:35:40.499757Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"         id  score\n0  001519c8    3.5\n1  0022f953    3.5\n2  0042269b    6.0\n3  0059420b    2.0\n4  0075873a    4.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>001519c8</td>\n      <td>3.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0022f953</td>\n      <td>3.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0042269b</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0059420b</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0075873a</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_logs.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T12:35:53.118511Z","iopub.execute_input":"2023-11-20T12:35:53.118935Z","iopub.status.idle":"2023-11-20T12:35:53.136736Z","shell.execute_reply.started":"2023-11-20T12:35:53.118900Z","shell.execute_reply":"2023-11-20T12:35:53.135441Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"         id  event_id  down_time  up_time  action_time activity down_event  \\\n0  0000aaaa         1     338433   338518           85    Input      Space   \n1  0000aaaa         2     760073   760160           87    Input      Space   \n2  2222bbbb         1     711956   712023           67    Input          q   \n3  2222bbbb         2     290502   290548           46    Input          q   \n4  4444cccc         1     635547   635641           94    Input      Space   \n\n  up_event text_change  cursor_position  word_count  \n0    Space                            0           0  \n1    Space                            1           0  \n2        q           q                0           1  \n3        q           q                1           1  \n4    Space                            0           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>event_id</th>\n      <th>down_time</th>\n      <th>up_time</th>\n      <th>action_time</th>\n      <th>activity</th>\n      <th>down_event</th>\n      <th>up_event</th>\n      <th>text_change</th>\n      <th>cursor_position</th>\n      <th>word_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>1</td>\n      <td>338433</td>\n      <td>338518</td>\n      <td>85</td>\n      <td>Input</td>\n      <td>Space</td>\n      <td>Space</td>\n      <td></td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0000aaaa</td>\n      <td>2</td>\n      <td>760073</td>\n      <td>760160</td>\n      <td>87</td>\n      <td>Input</td>\n      <td>Space</td>\n      <td>Space</td>\n      <td></td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2222bbbb</td>\n      <td>1</td>\n      <td>711956</td>\n      <td>712023</td>\n      <td>67</td>\n      <td>Input</td>\n      <td>q</td>\n      <td>q</td>\n      <td>q</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2222bbbb</td>\n      <td>2</td>\n      <td>290502</td>\n      <td>290548</td>\n      <td>46</td>\n      <td>Input</td>\n      <td>q</td>\n      <td>q</td>\n      <td>q</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4444cccc</td>\n      <td>1</td>\n      <td>635547</td>\n      <td>635641</td>\n      <td>94</td>\n      <td>Input</td>\n      <td>Space</td>\n      <td>Space</td>\n      <td></td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### Essay Constructor","metadata":{}},{"cell_type":"code","source":"class EssayConstructor:\n    def processingInputs(self, currTextInput):\n        # Where the essay content will be stored\n        essayText = \"\"\n        \n        # Produces the essay\n        for Input in currTextInput.values:\n            # If activity = Replace\n            if Input[0] == \"Replace\":\n                # splits text_change at ' => '\n                replaceTxt = Input[2].split(' => ')\n                # Done Touch\n                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayTxt[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n                continue\n                \n            # If activity = Paste\n            if Input[0] = \"Paste\":\n                # Done Touch\n                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayTxt[Input[1] - len(Input[2]):]\n                continue\n                \n            # If activity = Remove/Cut\n            if Input[0] == \"Remove/Cut\":\n                # Dont Touch\n                essayText = essayText[:Input[1]] + essayTxt[Input[1] - len(Input[2]):]\n                continue\n                \n            # If activity = Move...\n            if \"M\" in Input[0]:\n                # Gets rid of the \"Move from to\" text\n                croppedTxt = Input[0][10:]\n                # Splits cropped text by ' To '\n                splitTxt = croppedTxt.split(' To ')\n                # Splits split text again by ', ' for each item\n                valueArr = [item.split(', ') for item in splitTxt]\n                # Move from [2, 4] To [5, 7] = (2, 4, 5, 7)\n                moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n                # Skip if someone manages to activate this by moving to same place\n                if moveData[0] != moveData[2]:\n                    \n                ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (2) Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Function to construct essays copied from here (small adjustments): https://www.kaggle.com/code/yuriao/fast-essay-constructor\n\ndef processingInputs(currTextInput):\n  essayText = \"\"\n  for Input in currTextInput.values:\n    if Input[0] == 'Replace':\n      # splits text_change at ' => '\n      replaceTxt = Input[2].split(' => ')\n      essayText = essayText[:Input[1] - len(replaceTxt[1])] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n      continue\n\n    # If activity = Paste\n    if Input[0] == 'Paste':\n      essayText = essayText[:Input[1] - len(Input[2])] + essayText[Input[1] - len(Input[2]):]\n      continue\n\n    # If activity = Move ...\n    if \"M\" in Input[0]:\n      # Gets rid of the \"Move from to\" text\n      croppedTxt = Input[0][10:]\n      # Splits cropped text by ' To '\n      splitTxt = croppedTxt.aplit(' To ')\n      # Splits split text again by ', ' for each item\n      valueArr = [item.split(', ') for item in splitTxt]\n\n      # Move from [2, 4] To [5, 7] = (2, 4, 5, 7)\n      moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n\n      # Skip if someone manages to activiate this by moving to same place\n      if moveData[0] != moveData[2]:\n        # Check if they move text forward in essay (they are different)\n        if moveData[0] < moveData[2]:\n          essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n        else:\n          essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n\n      continue\n    \n    # If activity = input\n    essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n  return essayText\n\ndef getEssays(df):\n  # Copy required columns\n  textInputDf = copy.deepcopy(df[['id', 'activity', 'cursor_position', 'text_change']])\n  # Get rid of text inputs that make no change\n  textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']\n  # construct essay, fast\n  tqdm.pandas()\n  essay = textInputDf.groupby('id')[['activity', 'cursor_position', 'text_change']].progress_apply(lambda x: processingInputs(x))\n  # to dataframe\n  essayFrame = essay.to_frame().reset_index()\n  essayFrame.columns = ['id', 'essay']\n  \n  return essayFrame","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:02:59.093383Z","iopub.execute_input":"2023-11-19T05:02:59.093709Z","iopub.status.idle":"2023-11-19T05:02:59.111747Z","shell.execute_reply.started":"2023-11-19T05:02:59.093680Z","shell.execute_reply":"2023-11-19T05:02:59.110536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper function\n\ndef q1(x):\n  return x.quantile(0.25)\ndef q3(x):\n  return x.quantile(0.75)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:02:59.113570Z","iopub.execute_input":"2023-11-19T05:02:59.114349Z","iopub.status.idle":"2023-11-19T05:02:59.128501Z","shell.execute_reply.started":"2023-11-19T05:02:59.114278Z","shell.execute_reply":"2023-11-19T05:02:59.127559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AGGREGATIONS = ['count', 'mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', pd.DataFrame.kurt, 'sum']\n\ndef split_essays_into_sentences(df):\n  essay_df = df\n\n  essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!', x))\n  essay_df = essay_df.explode('sent')\n  essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n', '').strip())\n  # Number of characters in sentences\n  essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n  # Number of words in sentences\n  essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n  essay_df = essay_df[essay_df.sent_len != 0].reset_index(drop=True)\n  return essay_df\n\ndef compute_sentence_aggregations(df):\n  sent_agg_df = pd.concat([df[['id', 'sent_len']].groupby(['id']).agg(AGGREGATIONS),\n                            df[['id', 'sent_word_count']].groupby(['id']).agg(AGGREGATIONS)],\n                            axis=1)\n  sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n  sent_agg_df['id'] = sent_agg_df.index\n  sent_agg_df = sent_agg_df.reset_index(drop=True)\n  sent_agg_df.drop(columns=['sent_word_count_count'], inplace=True)\n  sent_agg_df = sent_agg_df.rename(columns={'sent_len_count': 'sent_count'})\n  return sent_agg_df\n\ndef split_essays_into_paragraphs(df):\n  essay_df = df\n\n  essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n  essay_df = essay_df.explode('paragraph')\n  # Number of characters in paragraphs\n  essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x))\n  # Number of words in paragraphs\n  essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n  essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n  return essay_df\n\ndef compute_paragraph_aggregations(df):\n  paragraph_agg_df = pd.concat(\n      [df[['id', 'paragraph_len']].groupby(['id']).agg(AGGREGATIONS),\n       df[['id', 'paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)],\n      axis=1\n  )\n  paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n  paragraph_agg_df['id'] = paragraph_agg_df.index\n  paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n  paragraph_agg_df.drop(columns=['paragraph_word_count_count'], inplace=True)\n  paragraph_agg_df = paragraph_agg_df.rename(columns={'paragraph_len_count': 'paragraph_count'})\n  return paragraph_agg_df\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:02:59.130033Z","iopub.execute_input":"2023-11-19T05:02:59.131040Z","iopub.status.idle":"2023-11-19T05:02:59.151653Z","shell.execute_reply.started":"2023-11-19T05:02:59.130996Z","shell.execute_reply":"2023-11-19T05:02:59.150615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sentence feature for train dataset\ntrain_sent_df = split_essays_into_sentences(train_essays)\ntrain_sent_agg_df = compute_sentence_aggregations(train_sent_df)\n\nplt.figure(figsize=(15, 1.5))\nplt.boxplot(x=train_sent_df.sent_len, vert=False, labels=['Sentence length'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:02:59.153419Z","iopub.execute_input":"2023-11-19T05:02:59.154074Z","iopub.status.idle":"2023-11-19T05:03:07.756992Z","shell.execute_reply.started":"2023-11-19T05:02:59.154030Z","shell.execute_reply":"2023-11-19T05:03:07.755099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Paragraph features for train dataset\ntrain_paragraph_df = split_essays_into_paragraphs(train_essays)\ntrain_paragraph_agg_df = compute_paragraph_aggregations(train_paragraph_df)\n\nplt.figure(figsize=(15, 1.5))\nplt.boxplot(x=train_paragraph_df.paragraph_len, vert=False, labels=['Paragraph length'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:03:07.758640Z","iopub.execute_input":"2023-11-19T05:03:07.759888Z","iopub.status.idle":"2023-11-19T05:03:16.194860Z","shell.execute_reply.started":"2023-11-19T05:03:07.759850Z","shell.execute_reply":"2023-11-19T05:03:16.193192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Features for test dataset\ntest_essays = getEssays(test_logs)\ntest_sent_agg_df = compute_sentence_aggregations(split_essays_into_sentences(test_essays))\ntest_paragraph_agg_df = compute_paragraph_aggregations(split_essays_into_paragraphs(test_essays))","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:03:16.201100Z","iopub.execute_input":"2023-11-19T05:03:16.201523Z","iopub.status.idle":"2023-11-19T05:03:16.294342Z","shell.execute_reply.started":"2023-11-19T05:03:16.201490Z","shell.execute_reply":"2023-11-19T05:03:16.293251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\n\nclass Preprocessor:\n    \n    def __init__(self, seed):\n        self.seed = seed\n        \n        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n        self.text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n        self.gaps = [1, 2, 3, 5, 10, 20, 50, 100]\n        \n        self.idf = defaultdict(float)\n    \n    def activity_counts(self, df):\n        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df['activity'].values):\n            items = list(Counter(li).items())\n            di = dict()\n            for k in self.activities:\n                di[k] = 0\n            for item in items:\n                k, v = item[0], item[1]\n                if k in di:\n                    di[k] = v\n            ret.append(di)\n        ret = pd.DataFrame(ret)\n        cols = [f'activity_{i}_count' for i in range(len(ret.columns))]\n        ret.columns = cols\n\n        cnts = ret.sum(1)\n\n        for col in cols:\n            if col in self.idf.keys():\n                idf = self.idf[col]\n            else:\n                idf = df.shape[0] / (ret[col].sum() + 1)\n                idf = np.log(idf)\n                self.idf[col] = idf\n\n            ret[col] = 1 + np.log(ret[col] / cnts)\n            ret[col] *= idf\n\n        return ret\n\n    def event_counts(self, df, colname):\n        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df[colname].values):\n            items = list(Counter(li).items())\n            di = dict()\n            for k in self.events:\n                di[k] = 0\n            for item in items:\n                k, v = item[0], item[1]\n                if k in di:\n                    di[k] = v\n            ret.append(di)\n        ret = pd.DataFrame(ret)\n        cols = [f'{colname}_{i}_count' for i in range(len(ret.columns))]\n        ret.columns = cols\n\n        cnts = ret.sum(1)\n\n        for col in cols:\n            if col in self.idf.keys():\n                idf = self.idf[col]\n            else:\n                idf = df.shape[0] / (ret[col].sum() + 1)\n                idf = np.log(idf)\n                self.idf[col] = idf\n            \n            ret[col] = 1 + np.log(ret[col] / cnts)\n            ret[col] *= idf\n\n        return ret\n\n    def text_change_counts(self, df):\n        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df['text_change'].values):\n            items = list(Counter(li).items())\n            di = dict()\n            for k in self.text_changes:\n                di[k] = 0\n            for item in items:\n                k, v = item[0], item[1]\n                if k in di:\n                    di[k] = v\n            ret.append(di)\n        ret = pd.DataFrame(ret)\n        cols = [f'text_change_{i}_count' for i in range(len(ret.columns))]\n        ret.columns = cols\n\n        cnts = ret.sum(1)\n\n        for col in cols:\n            if col in self.idf.keys():\n                idf = self.idf[col]\n            else:\n                idf = df.shape[0] / (ret[col].sum() + 1)\n                idf = np.log(idf)\n                self.idf[col] = idf\n            \n            ret[col] = 1 + np.log(ret[col] / cnts)\n            ret[col] *= idf\n            \n        return ret\n\n    def match_punctuations(self, df):\n        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n        ret = list()\n        for li in tqdm(tmp_df['down_event'].values):\n            cnt = 0\n            items = list(Counter(li).items())\n            for item in items:\n                k, v = item[0], item[1]\n                if k in self.punctuations:\n                    cnt += v\n            ret.append(cnt)\n        ret = pd.DataFrame({'punct_cnt': ret})\n        return ret\n\n    def get_input_words(self, df):\n        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n        tmp_df.drop(['text_change'], axis=1, inplace=True)\n        return tmp_df\n    \n    def make_feats(self, df):\n        \n        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n        \n        print(\"Engineering time data\")\n        for gap in self.gaps:\n            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n\n        print(\"Engineering cursor position data\")\n        for gap in self.gaps:\n            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n\n        print(\"Engineering word count data\")\n        for gap in self.gaps:\n            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)\n        \n        print(\"Engineering statistical summaries for features\")\n        feats_stat = [\n            ('event_id', ['max']),\n            ('up_time', ['max']),\n            ('action_time', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n            ('activity', ['nunique']),\n            ('down_event', ['nunique']),\n            ('up_event', ['nunique']),\n            ('text_change', ['nunique']),\n            ('cursor_position', ['nunique', 'max', 'quantile', 'sem', 'mean']),\n            ('word_count', ['nunique', 'max', 'quantile', 'sem', 'mean'])]\n        for gap in self.gaps:\n            feats_stat.extend([\n                (f'action_time_gap{gap}', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n                (f'cursor_position_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n                (f'word_count_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt])\n            ])\n        \n        pbar = tqdm(feats_stat)\n        for item in pbar:\n            colname, methods = item[0], item[1]\n            for method in methods:\n                pbar.set_postfix()\n                if isinstance(method, str):\n                    method_name = method\n                else:\n                    method_name = method.__name__\n                pbar.set_postfix(column=colname, method=method_name)\n                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n                feats = feats.merge(tmp_df, on='id', how='left')\n\n        print(\"Engineering activity counts data\")\n        tmp_df = self.activity_counts(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n        \n        print(\"Engineering event counts data\")\n        tmp_df = self.event_counts(df, 'down_event')\n        feats = pd.concat([feats, tmp_df], axis=1)\n        tmp_df = self.event_counts(df, 'up_event')\n        feats = pd.concat([feats, tmp_df], axis=1)\n        \n        print(\"Engineering text change counts data\")\n        tmp_df = self.text_change_counts(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n        \n        print(\"Engineering punctuation counts data\")\n        tmp_df = self.match_punctuations(df)\n        feats = pd.concat([feats, tmp_df], axis=1)\n\n        print(\"Engineering input words data\")\n        tmp_df = self.get_input_words(df)\n        feats = pd.merge(feats, tmp_df, on='id', how='left')\n\n        print(\"Engineering ratios data\")\n        feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n        feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n        feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n        feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['up_time_max']\n\n        return feats","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:03:16.296034Z","iopub.execute_input":"2023-11-19T05:03:16.296454Z","iopub.status.idle":"2023-11-19T05:03:16.417067Z","shell.execute_reply.started":"2023-11-19T05:03:16.296409Z","shell.execute_reply":"2023-11-19T05:03:16.415425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessor = Preprocessor(seed=42)\ntrain_feats = preprocessor.make_feats(train_logs)\ntest_feats = preprocessor.make_feats(test_logs)\nnan_cols = train_feats.columns[train_feats.isna().any()].tolist()\ntrain_feats = train_feats.drop(columns=nan_cols)\ntest_feats = test_feats.drop(columns=nan_cols)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:03:16.419080Z","iopub.execute_input":"2023-11-19T05:03:16.419738Z","iopub.status.idle":"2023-11-19T05:09:34.584215Z","shell.execute_reply.started":"2023-11-19T05:03:16.419702Z","shell.execute_reply":"2023-11-19T05:09:34.583056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Code for additional aggregations comes from here: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs\n\ntrain_agg_fe_df = train_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(\n    ['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\ntrain_agg_fe_df.columns = ['_'.join(x) for x in train_agg_fe_df.columns]\ntrain_agg_fe_df = train_agg_fe_df.add_prefix(\"tmp_\")\ntrain_agg_fe_df.reset_index(inplace=True)\n\ntest_agg_fe_df = test_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(\n    ['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\ntest_agg_fe_df.columns = ['_'.join(x) for x in test_agg_fe_df.columns]\ntest_agg_fe_df = test_agg_fe_df.add_prefix(\"tmp_\")\ntest_agg_fe_df.reset_index(inplace=True)\n\ntrain_feats = train_feats.merge(train_agg_fe_df, on='id', how='left')\ntest_feats = test_feats.merge(test_agg_fe_df, on='id', how='left')","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:09:34.586014Z","iopub.execute_input":"2023-11-19T05:09:34.586550Z","iopub.status.idle":"2023-11-19T05:09:40.361237Z","shell.execute_reply.started":"2023-11-19T05:09:34.586515Z","shell.execute_reply":"2023-11-19T05:09:40.360056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Code for creating these features comes from here: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs\n# Idea is based on features introduced in Section 3 of this research paper: https://files.eric.ed.gov/fulltext/ED592674.pdf\n\ndata = []\n\nfor logs in [train_logs, test_logs]:\n    logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n    logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n    \n    group = logs.groupby('id')['time_diff']\n    largest_lantency = group.max()\n    smallest_lantency = group.min()\n    median_lantency = group.median()\n    initial_pause = logs.groupby('id')['down_time'].first() / 1000\n    pauses_half_sec = group.apply(lambda x: ((x>0.5) & (x<1)).sum())\n    pauses_1_sec = group.apply(lambda x: ((x>1) & (x<1.5)).sum())\n    pauses_1_half_sec = group.apply(lambda x: ((x>1.5) & (x<2)).sum())\n    pauses_2_sec = group.apply(lambda x: ((x>2) & (x<3)).sum())\n    pauses_3_sec = group.apply(lambda x: (x>3).sum())\n    \n    data.append(pd.DataFrame({\n        'id': logs['id'].unique(),\n        'largest_lantency': largest_lantency,\n        'smallest_lantency': smallest_lantency,\n        'median_lantency': median_lantency,\n        'initial_pause': initial_pause,\n        'pauses_half_sec': pauses_half_sec,\n        'pauses_1_sec': pauses_1_sec,\n        'pauses_1_half_sec': pauses_1_half_sec,\n        'pauses_2_sec': pauses_2_sec,\n        'pauses_3_sec': pauses_3_sec,\n    }).reset_index(drop=True))\n\ntrain_eD592674, test_eD592674 = data\n\ntrain_feats = train_feats.merge(train_eD592674, on='id', how='left')\ntest_feats = test_feats.merge(test_eD592674, on='id', how='left')\ntrain_feats = train_feats.merge(train_scores, on='id', how='left')","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:09:40.362763Z","iopub.execute_input":"2023-11-19T05:09:40.363182Z","iopub.status.idle":"2023-11-19T05:09:52.283410Z","shell.execute_reply.started":"2023-11-19T05:09:40.363152Z","shell.execute_reply":"2023-11-19T05:09:52.282246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding the additional features to the original feature set\n\ntrain_feats = train_feats.merge(train_sent_agg_df, on='id', how='left')\ntrain_feats = train_feats.merge(train_paragraph_agg_df, on='id', how='left')\ntest_feats = test_feats.merge(test_sent_agg_df, on='id', how='left')\ntest_feats = test_feats.merge(test_paragraph_agg_df, on='id', how='left')","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:09:52.285036Z","iopub.execute_input":"2023-11-19T05:09:52.285402Z","iopub.status.idle":"2023-11-19T05:09:52.323542Z","shell.execute_reply.started":"2023-11-19T05:09:52.285371Z","shell.execute_reply":"2023-11-19T05:09:52.322106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_col = ['score']\ndrop_cols = ['id']\ntrain_cols = [col for col in train_feats.columns if col not in target_col + drop_cols]","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:09:52.325597Z","iopub.execute_input":"2023-11-19T05:09:52.326038Z","iopub.status.idle":"2023-11-19T05:09:52.331439Z","shell.execute_reply.started":"2023-11-19T05:09:52.326005Z","shell.execute_reply":"2023-11-19T05:09:52.330295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (3) LightGBM train and predic","metadata":{}},{"cell_type":"code","source":"OOF_PREDS = np.zeros((len(train_feats), 2))\nTEST_PREDS = np.zeros((len(test_feats), 2))","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:09:52.332713Z","iopub.execute_input":"2023-11-19T05:09:52.333396Z","iopub.status.idle":"2023-11-19T05:09:52.347154Z","shell.execute_reply.started":"2023-11-19T05:09:52.333351Z","shell.execute_reply":"2023-11-19T05:09:52.345925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Code comes from here: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs\n\nmodels_dict = {}\nscores = []\n\ntest_predict_list = []\nbest_params = {'reg_alpha': 0.007678095440286993, \n               'reg_lambda': 0.34230534302168353, \n               'colsample_bytree': 0.627061253588415, \n               'subsample': 0.854942238828458, \n               'learning_rate': 0.038697981947473245, \n               'num_leaves': 22, \n               'max_depth': 37, \n               'min_child_samples': 18,\n               'n_jobs':4\n              }\n\nfor i in range(5): \n    kf = model_selection.KFold(n_splits=10, random_state=42 + i, shuffle=True)\n    oof_valid_preds = np.zeros(train_feats.shape[0])\n    X_test = test_feats[train_cols]\n    for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n        \n        X_train, y_train = train_feats.iloc[train_idx][train_cols], train_feats.iloc[train_idx][target_col]\n        X_valid, y_valid = train_feats.iloc[valid_idx][train_cols], train_feats.iloc[valid_idx][target_col]\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            'random_state': 42,\n            \"n_estimators\" : 12001,\n            \"verbosity\": -1,\n            **best_params\n        }\n        model = lgb.LGBMRegressor(**params)\n        early_stopping_callback = lgb.early_stopping(200, first_metric_only=True, verbose=False)\n        \n        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],  \n                  callbacks=[early_stopping_callback],\n        )\n        valid_predict = model.predict(X_valid)\n        oof_valid_preds[valid_idx] = valid_predict\n        OOF_PREDS[valid_idx, 0] += valid_predict / 5\n        test_predict = model.predict(X_test)\n        TEST_PREDS[:, 0] += test_predict / 5 / 10\n        test_predict_list.append(test_predict)\n        score = metrics.mean_squared_error(y_valid, valid_predict, squared=False)\n        models_dict[f'{fold}_{i}'] = model\n\n    oof_score = metrics.mean_squared_error(train_feats[target_col], oof_valid_preds, squared=False)\n    scores.append(oof_score)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:09:52.348987Z","iopub.execute_input":"2023-11-19T05:09:52.349439Z","iopub.status.idle":"2023-11-19T05:16:50.675506Z","shell.execute_reply.started":"2023-11-19T05:09:52.349405Z","shell.execute_reply":"2023-11-19T05:16:50.674113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('OOF metric LGBM = {:.5f}'.format(metrics.mean_squared_error(train_feats[target_col], OOF_PREDS[:, 0], squared=False)))","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:16:50.676990Z","iopub.execute_input":"2023-11-19T05:16:50.677472Z","iopub.status.idle":"2023-11-19T05:16:50.688228Z","shell.execute_reply.started":"2023-11-19T05:16:50.677430Z","shell.execute_reply":"2023-11-19T05:16:50.686928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (4) LightAutoML NN (DenseLight) prediction","metadata":{}},{"cell_type":"code","source":"from lightautoml.automl.presets.tabular_presets import TabularAutoML\nfrom lightautoml.tasks import Task\nimport joblib","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:16:50.690223Z","iopub.execute_input":"2023-11-19T05:16:50.690700Z","iopub.status.idle":"2023-11-19T05:17:34.185180Z","shell.execute_reply.started":"2023-11-19T05:16:50.690659Z","shell.execute_reply":"2023-11-19T05:17:34.184024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(3):\n    oof_pred, automl = joblib.load('/kaggle/input/linkinglamamodels/oof_and_lama_denselight_{}.pkl'.format(i))\n    OOF_PREDS[:, 1] += oof_pred / 3\n    TEST_PREDS[:, 1] += automl.predict(test_feats[train_cols]).data[:, 0] / 3","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:17:34.186927Z","iopub.execute_input":"2023-11-19T05:17:34.187793Z","iopub.status.idle":"2023-11-19T05:18:32.243034Z","shell.execute_reply.started":"2023-11-19T05:17:34.187750Z","shell.execute_reply":"2023-11-19T05:18:32.241741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('OOF metric LightAutoML_NN = {:.5f}'.format(metrics.mean_squared_error(train_feats[target_col], OOF_PREDS[:, 1], squared=False)))","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:18:32.246514Z","iopub.execute_input":"2023-11-19T05:18:32.246885Z","iopub.status.idle":"2023-11-19T05:18:32.255389Z","shell.execute_reply.started":"2023-11-19T05:18:32.246854Z","shell.execute_reply":"2023-11-19T05:18:32.254536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (5) Blending","metadata":{}},{"cell_type":"code","source":"best_sc = 1\nfor w in np.arange(0, 1.01, 0.001):\n    sc = metrics.mean_squared_error(train_feats[target_col], \n                                    w * OOF_PREDS[:, 0] + (1-w) * OOF_PREDS[:, 1], \n                                    squared=False)\n    if sc < best_sc:\n        best_sc = sc\n        best_w = w\n        \nprint('Composition OOF score = {:.5f}'.format(best_sc))\nprint('Composition best W = {:.3f}'.format(best_w))","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:18:32.256796Z","iopub.execute_input":"2023-11-19T05:18:32.257388Z","iopub.status.idle":"2023-11-19T05:18:34.440133Z","shell.execute_reply.started":"2023-11-19T05:18:32.257354Z","shell.execute_reply":"2023-11-19T05:18:34.438968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (6) Submission creation","metadata":{}},{"cell_type":"code","source":"W = [best_w, 1 - best_w]\ntest_preds = TEST_PREDS[:, 0] * W[0] + TEST_PREDS[:, 1] * W[1]\ntest_preds","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:18:34.442061Z","iopub.execute_input":"2023-11-19T05:18:34.442868Z","iopub.status.idle":"2023-11-19T05:18:34.452192Z","shell.execute_reply.started":"2023-11-19T05:18:34.442824Z","shell.execute_reply":"2023-11-19T05:18:34.451123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_feats['score'] = test_preds\ntest_feats[['id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:18:34.453741Z","iopub.execute_input":"2023-11-19T05:18:34.454068Z","iopub.status.idle":"2023-11-19T05:18:34.470718Z","shell.execute_reply.started":"2023-11-19T05:18:34.454041Z","shell.execute_reply":"2023-11-19T05:18:34.469249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_feats[['id', 'score']]","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:18:34.472727Z","iopub.execute_input":"2023-11-19T05:18:34.473087Z","iopub.status.idle":"2023-11-19T05:18:34.494146Z","shell.execute_reply.started":"2023-11-19T05:18:34.473056Z","shell.execute_reply":"2023-11-19T05:18:34.493205Z"},"trusted":true},"execution_count":null,"outputs":[]}]}